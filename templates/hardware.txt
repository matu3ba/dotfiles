====cpu
====general
====specification_langs
====computing
====debugging
====tooling
====nvm
====bad_designs
====benchmarks
====physics
====ai_hardware
====hdl_synthesis
====timing
====chiplet_standards
====architecure

https://github.com/MarcusCemes/advent-of-code-2025

Hard questions
2. Explain verification process of safety-critical product + design trade-offs.
How does safety level affect effort+costs?
Related
Overview of safety-critical product requirements for different safety levels of different domains
with tooling+product certification process.

====cpu
https://www.ccbp.in/blog/articles/bus-structure-in-computer-architecture
* internal CPU bus is a Parallel Bus

NUMA Architecture
* lscpu | grep "NUMA"
* chiplets distributed on NUMA-nodes
  - L1, L2 cache local
  - L3 cache likely remote/higher latency
* interconnect
  - UCIe throughput, BMI for low latency, low throughput (heat)
  => "RCAS: Adaptive Runtime System for Chiplet-Aware Scheduling"
  => "Optimizing Sorting for Chiplet-Based CPUs"
  => "A Performance Analysis of Chiplet-Based Systems"
     - "may give away over a 30% of the monolithic performance"
  => "MEMPLEX: A Memory System with Replication and Migration of Data for Multi-Chiplet NUMA Architectures"
     - +10%
  - alternatives: Infinity Fabric (AMD), InfiniBand (HPC)

* "Design of hierarchical cache coherence protocol based on Chiplet architecture"
* intel a few years ago: meshes with centeral chiplets be l3, outer be cores;
  mesh between everything, connections for chiplets was on corner (would be
  mirrored/flipped as needed); bus can send in any cardinal direction, which
  would go to l3 or core
  - connection with mesh broken => rotate the cores if needed
  - before mesh buses: ring buses, where you have inner part of the ring is l3 and outer
    is chiplets
  - each chiplet has NIC for mesh bus

====specification_langs
* classical HDLs
  - VHDL
  - Verilog
* simulation langs
  - SystemC/TLM (Verilator for hdl->c++/systemc translation for cycle-accurate model)
    - works with rr
  - SystemC-AMS
* meta langs for VHDL/Verilog codegen (also HDL)
  - ACT
  - Amaranth
  - Spinal
  - Chisel (from 2012)
  - Clash (BSD2, based on Haskell, repl, targets VHDL, Verilog, or SystemVerilog)
  - Bluespec
  - Spade
  - Hardcaml (encouraged)
    + https://github.com/asinghani/advent-of-hardcaml-2025
  - DFHDL
  - Filament
    + interfaces to specify temporal and structural constrains (not existent in VHDL etc)
    + like chisel but with well balanced pipelines
    + "Fearless Hardware Design" - Rachit Nigam (Latch-Up 2023)"
    + https://github.com/cucapra/filament
* atopile https://github.com/atopile/atopile
  - small scale hardware dev suite

====general
* definitions
  * HDL: lang with subset that has spec to describe synthesizable logical circuits
    + Verilog, Chisel, VHDL, ACT
    - C, Scala, Python, HTML
  * RTTL: hdl only describing RTL code limited to composition of:
    wires, combinational logic, registers, macros (black boxes)
    + SpinalHDL, Chisel, Amaranth, Bluespec
    - Verilog, VHDL, ACT (do not exclusively allow RTL description)
  * DSL: leverages host syntax and ecosystem to create lib/API for specific application
    + SpinalHDL, Chisel, Amaranth
    - Verilog, VHDL, ACT, Bluespec
  * HLS: tool to transform non-HDL into HW or existing HW to different HW without
    functional changes
    + C -> Verilog, Matlab -> Verilog
    - Chisel
  * HDL model essentials: time<->state, concurrency, data validity
    * dataflow: functional
    * register transfer: cycle accurate
    * event driven: event based
  * HDL full/no control vs no/full ecosystem
  * DFHDL claims having both (for Scala 3)
* cpu/gpu news https://semiwiki.com/
* avoid or mitigate Common-Mode Noise (CMN) / Gleichtaktstörung
  + measurable +SRC or -SRC against GND
  + can be up to ~100V AC CMR for 12V USB laptop charger
  + workarounds:
    - use power supply without CMR
    - always measure before usage
    - connect -SRC of power supply with GND / ref GND of power supply with GND,
    either with capacitor or direct connection
      o connect with for example power supply, power socket etc
  + problematic with laptops, no problem with grounded desktop pc
  + correct hardware design more complex/expensive, simpler to provide GND
  see also https://electronics.stackexchange.com/questions/79752/what-is-common-mode-noise
* fault injection at HDL source, netlist, physical database, bitstream
  other debugging techniques?
* classical hardware/VLSI design lvls (https://www.guvi.in/blog/5-levels-in-vlsi-design/)
  https://www.guvi.in/mlp/VLSI-design-and-verification
  * 1 architecture (process with goals)
    - process/high level system simulation via Matlab, Simulink, SystemC (TLM2.0)
    - defines functionality, perf, power constrains, etc
    - includes decisions on memory, processing units, protocols, timing, caches,
      testing, validation, debugging, etc
  * 2 functional (HDL)
    - Verilog, VHDL (hardware) simulators like ModelSim, Xilinx, Vivado, Synopsys VCS
    - simulation via testbenches and assertion-based validation
  * 3 logic/gate (netlist)
    - netlist synthesized from tools like Synopsys Design Compiler, Cadence Genus
      * FSM for sequential logic
      * HDL is optimized according to speed, perf, area
    - gate-level simulations to validate correctness of logic cirtuits
    - Register Transfer Level (RTL) code written to define how data flows through system
    - simulation via https://github.com/emsec/hal, SPICE
  * 4 circuit (schematics)
    - logical gates are converted to cirtuit schematics using transistors
    - transistor level simulation with SPICE
    - schematic entry and layout simulation with tools like Cadence Virtuoso, Synopsys Custom Designer
    - power dissipation, signal integrity, voltage levels, circuit operations are analyzed
  * 5 physical
    - floorplanning, placing, routing (connect components with wires), clock
      tree synthesis (build clock distribution network), power planning
      via place blocks, macros, standard cells
    - layout design tools like Cadence Innovus, Synopsys IC Compiler
    - Design Rule Check, Layout vs. Schematic (LVS) tools like ?
    - fabrication process specific tools and adjustments
  * 6 production tooling, coordination, process, assembly, packaging, tracking, etc
  * 7 hardware bringup, checks, testing, validation, tracking, etc
  * 8 software, product, customer coordination, integration
* debugging questions
  * what tools exist to do simulate/validate 1+2,1+3, etc?
  * best practice overview for design for test (insert scan chains and test logic)?
  * where does static time analysis (STA) to ensure design meets timing requirements
    fit in? each step?
  * best practice overview "physical verification" (check layout design rules,
    compare layout and netlist)

https://teramesh.tech/Resources/Knowledge-Base/Signal-Integrity-Analysis-Methodology-for-Printed-Circuit-Boards/
https://teramesh.tech/Resources/#FAQ
* Cadence Sigrity Aurora/Clarity
* Siemens HyperLynx (excellent at prelayout modeling + topology design, excellent breakdown of transmission line loss)
* ANSYS HFSS (benchmark for 3D optimizations)
* Keysight ADS (great at complex topology analysis, monte Carlo, etc)
* Simulink/MATLAB

* VLSI fgpa flow
  + TODO

https://www.infoq.com/presentations/computing-interaction-technology/
TODO summarize future hardware

====computing
classical CPUs are hardware based instruction interpreters

====debugging
- protect data transmission of unshielded cables
  * use at least CRC
  * certificate authorities will test for correct error behavior
  * check the regulatory standards and laws

- parallel IO Ports, parallel input ports, parallel output ports
  * architecture vendor does not specify "how parallel data is received or send",
    but may limit it via cache misses, interrupts etc
  * board/peripherals vendor specifies
  * can be very limiting and fundamental bottleneck
  * writes may be blocking and thus insufficient for time-deterministic use cases
  * see also "Parallel Input/Output", "ARM Cortex M7 parallel io ports"
  * fully parallel and deterministic IO throughput usually means "uses a PIC"
    - PIC known to have best peripherals support from microcontrollers

- debugging capabilities of host and target system often not exemplified
  * tooling takes effort to produce and maintain
  * custom logic often IP, so vendors may prefer to not give out information
    unless via sold debugging tooling

Security Level (SL)
Safety Integrity Level (SIL)
Common Criteria with Evaluation Assurance Levels (EAL)

Risk cube
* x probability and/or frequence of failure
* y severity of consequence
* z probability and/or frequency of dangerous event
Then one can specify
* SIL specifies xy relation (failure x severity)
* EAL specifies yz relation (severity x dangerous event)
* risk class specifies xz relation (failure x dangerous event)
to decide tradeoffs.

"What’s Security Level got to do with Safety Integrity Level?" by Jens Braband
SL expands CIA (data confidentiality, integrity, availability) into
1. identification and authentication control (IAC)
2. use control (UC)
3. system integrity (SI)
4. data confidentiality (DC)
5. restricted data flow (RDF)
6. timely response to events (TRE)
7. resource availability (RA)
with integrity being IAC, UC, SI, and TRE; availability being RA and
confidentiality being DC and RDF with each area getting a value of 1 to 4
to form a vector.
SL1 Protection against casual or coincidental violation
SL2 Protection against intentional violation using simple means with few resources, generic skills and a low degree of motivation
SL3 Protection against intentional violation using sophisticated means with moderate resources, IACS-specific skills and a moderate degree of motivation
SL4 Protection against intentional violation using sophisticated means with extended resources, IACS-specific skills and a high degree of motivation
It is recommended to always have SL1 for safety-related systems.

The Security Assurance Level SAL is specified in a similar way as SIL.

- practical Safety Integrity Level 4 (SIL4) systems
  * Azure RTOS
- practical EAL7 systems
  * SeL4

Generally, EAL means how correct the code can be relied on to handle faults/hazardous events.
Similarly, SIL means how reliable the functionality can avert the worst specific consequences.
The risk class(es) combine(s) different severities of dangerous events.

https://interrupt.memfault.com/blog/schematic-review-checklist
As software engineer you must ensure the device is sufficiently correct to diagnose
1. hardware and 2. software problems to reduce risk and costs.
This requires eliminating fatal device failure classes like
- 0. flash and reset strategy: ensure you can not hard brick the device by accident
- 1. brown-out loops: low battery with low voltage cutoff shuts down device
- 2. reset by power drain: correct watchdog(s) functionality on hardware and software
- 3. missing I2C reset: resetting individual devices to work around hardware and software issues
- 4. missing or unreadable GPIO maps/peripherals: unclear peripherals lead to
  bugs like (old) hardware versions being wrongly connected with superflous oscillosope usage etc
- 5. missing strategy for debugging in the field: JTAG and alike are nice, but often useless in field
  and reproducing locally is often very hard without data. No JTAG can be painful, when local performance
  is essential

Power/Battery
    * shutdown precedur of batter powered device?
    * battery powered: firmware knows power level and to what accuracy (needed)?
    * how does user reset device?
GPIOs
    * GPIO map in spreadsheet, main fn after reset, intended fn, additional noes Microcontroller Unit (MCU) peripherals used only once? MCU in reset, are peripheral devices held in proper state? (pullup/down etc)
    * reset line with proper pullup/down
    * debug GPIO used only for debug purpose?
Busses
    * (if necessary) mechanism to reset IC's attached to bus avaiable?
    * all I2C devices on same bus or different addresses?
    * MCU reset in middle of I2C read, how does recovering to talk again to other I2C device work?
    * MCU reset in SPI transfer, how does receiver handle that?
    * UART devices: Rx/Tx setup correct?
    * UART devices: RTS/CTS lines needed?
    * UART devices: RTS/CTS lines setup correctly? (some devices have not RTS -> CTS)
Memory/Flash Storage
    * code size estimate? RAM estimate? how much buffer if estimate is low?
    * sufficient space for more features over lifetime of product?
    * crash logs storage? (usually need to survive system reset)
Debug/Test Infra
    * how firmware engineer attaches debugger to board? soldering usually bad answer
    * non-essential helpful GPIOs exposed for debugger (ie SWO or ETM)?
    * existing test points for power rails, analog signals, busses?

====tooling
architecture: Control logic (Simulink), Connected Hardware (TLM2.0 at RTL )
* tlm2.0
  based on "A Systematic Mapping Study on SystemC/TLM Modeling Capabilities in New Research Domains"
  - Transaction Level model (above pin=fn layer) for virtual platform models (SoC,Buses)
    via fn calls instead of pin/cycle-accurate bus protocols (faster simulation compared to RTL designs)
  - use cases
    + Software dev (UT/LT) [coarsest modelling granularity]
      => idea
    + Architecture exploration including performance analysis (LT-CT)
      => protocol-agnostic hw model for reasonable timing approximation (bandwidth, latency)
      => timing approximate perf of interconnect and important peripherals (memory, accelerators)
      => alternatives 1 instruction set simulators, 2 full-system and arch simulators
         * 1 QEMU with binary translation
         * 1 Spike with highly efficient caching interpreter-based ISS
         * 2 Renode with multiple embedded system simulation within multi-node networks
         * 2 Gem5 with models of multiple processor archs and memory hierachies and
           cycle accurate simulation
         * main drawbacks: integration process complex and time-consuming
    + Power estimation (LT-RTL)
      => classified based on dependence on input data and profiling methodologies
      => trade-offs
         * input-dependent
           - dynamic simulation, signal-activity-based estimation, behavioral modeling
         * input-agnostic
           - static analysis, architecture-level estimation
         * profiled
           - run-time data/measurements from real/simulated execution (from input-dependent)
         * non-profiled
           - analytical models, predefined benchmarks
      => offers instrumenting dynamic voltage and frequency scaling (DVFS)
      => challenging task: accurate approximation of power artifacts from
         lower abstraction levels
      => lower abstraction levels: non-profiled RTL model cannot use info regarding
         target standard library, implementation technology etc
      => early design stage security heavily relies on input-dependent characteristics
         of power consumption
      => scaling issues arise, when subsystems or components have lot of input combinations
    + Fault-Injection analysis (AT-RTL)
      => IC dependability crucial as it covers security, safety and reliability attributes
      => few SystemC-based studies address security
      => fault injection can be done at gate level netlist, RTL or Eletronic
         System Level (ESL)
      => ISO 26262 considers fault injection an effective approach for validating
         the safety requirements of safety-critical systems and accepts only
         gate-level netlist as satisfactory level; RTL acceptable as long as correlation
         between RTL and gate level is considered
      => practice: Fault-injection techniques (FIT). Discovering faults in gate-level
         netlist costly and time-consuming due to 1 enormous changes within IC design
         that safety-related mechanism may need, 2 enormous fault space as gate-level
         netlist provides a detailed implementation of IC design
      => faults can occur in all stages of IC design process (spec, design, dev,
         manufacturing, assembly, installation, operational lifetime) and non-elimination
         decreases reliability
      => for shorter dev time and faster simulation with fewer impl details, TLM models
         can be used to evaluate fault propagation in IC designs
      => Permanent faults, transient faults, intermittent faults
      => Hardware-based FIT
         * high time reolution, risk of permanent system damage, limit set of injectable
           faults, require specialized hardware
      => Simulation-based FIT
         * vary by abstraction level; TLM up to 100x speedup compared to RTL simulation
      => Emulation-based FIT
         * require synthesizable, optimized hardware description model and platform
           and like hw-based FIT effectively access fault tolerance, they typically
           viable only later in the SoC dev cycle
      => high-level fault injection (HLFI) for extracting critical system params
         (computation time, parameter vars) in turn for faster simulations
         and more effective filtering of faulty behaviors while considering
         system specification
    + Functional and Security Verification (LT-RTL) [verification means functionality eval, no math solving]
      => verification is up to 50% of the overall budget in time and human resources
      => Hardware Trojan (HT) risk increased by use of third-party IPs
      => HLS as possible productive alternative to RTL by allowing automatic RTL
         generation from SystemC; so more IPs are expected to be delivered as
         SystemC HLS (high level synthesis) designs
      => highlights importance of verifying SystemC designs against HTs
      => many methods, but formal model construction complicated by SystemC
         event-driven semantics/design;
         fuzzing looks promising, so far no annotations for variable inputs
         to combine with fuzzing (symbolic fuzzing);
         unclear if approaches used in distributed databases could work or sync
         issues are simply not that relevant (see Jepsen)
    + Side-Channel Analysis (CT-RTL) [finest modelling granularity]
      => evaluating physical phenomena (power, timing or electro-magnetic) and
         mathematical criteria to qualify impact on leakage (Application, Kernel
         influence not respected/information not available to app/kernel)
      => example test vector leakage assessment (TVLA)
      => most tooling is post-synthesis
      => requires very accurate timing and power estimations
      => best practice defense: compute all, choose one; introduce noise;
         isolate device; reset on anomalies
  - components
    + modules
      * TODO
    + processes
      * threads called once during simulation, can be halted
      * methods called multiple times during simulation, can not be halted
    + time and clock (sync of processes with clocks on positive/negative edge)
    + events and sensitivity (sensitivity of process determinites if it is
      resumed or activated to distinct collection of events)
    + interfaces and channels for module communication
      * interface set of method declarations how to use communication channel
      * channel serves as container for communication functionality
    + ports
      * port is object through which module can access channel's interface and
        can have 1 of 3 directions: input, output, bidirectional
  - foundational building blocks mechanisms
    + (non)blocking interfaces, DMI, Quantum keeper, Sockets, Generic payload, Phases,
      Non-blocking interfaces, Extensions
  - coding styles (guidelines as suggestions, not normative)
    + untimed (UT)
      => bus-based systems need concept of time to model software on SoC, means
         models containing restricted timing information of unspecified accuracy
    + loosely-timed (LT) => as fast as possible, sufficient timing to boot OS + multi-core system
      => temporal decoupling by system/process runs ahead of simulation time
      => each transaction completes in 1 fn call
      => uses DMI gives models or simulators (CPUs or ISA simulators) bypassing
    + loosely-timed contention-aware (LT-CA)
      => combines simulation speed of LT with memory contention accuracy of AT models
      => offers fast simulation and accurate memory contention monitoring
      => further validation across memory-intensive applications necessary for
         broader applicability
    + approximately timed (AT) => just enough info for perf modeling, meaning
      cycle-approximate or cycle-count-accurate; meaning differs between purpose/domains
      => sufficient for architecture exploration
      => processes run in lock-step in simulation time
      => each transaction has 4 timing points (extensible)
    + Cycle-accurate/Cycle-timed (CT)
      => replicates behavior of hardware design at individual clock cycles,
         updates system state with every clock cycle (when access to state is needed),
         possible to predict model state at any given cycle at its external boundary,
         1-to-1 correspondence between states and external observable states of
         corresponding RTL model
    + Register-transfer level (RTL)
      => behavior description in terms of data flow between registers and logical ops
         (combinational logic) on data, RTL models offer more details due to
         structural description of the circuit (exact logic, data paths, individual
         registers), so they are more suitable for synthesis into hardware
  - interoperability layer
    + allows models from different sources to play together
    + 1. core interfaces and sockets between
      => b_transport(TRANS&, sc_time&)
        - completes in 1 call, fw only, used with loosely timed coding style
      => nb_transport_fw/bw(TRANS&, PHASE&, sc_time&)
        - allows multiple back-forth calls, used with approximately timed coding style
      => transport_dbg(TRANS&)
        - zero delay, no ctx switches, no side effects
      => get/invalidate_direct_mem_ptr(TRANS&, tlm_dmi&/sc_dt::uint64 start/end_range)
        - fw/bw path, request memory from target/invalidates memory granted from target
      => Generic Payload
        - tlm_command, address(uint64), data_ptr(char*), data_len(uint),
          kbyte_enable_ptr(char*), byte_enable_len(uint), stream_width(uint),
          dmi_hint(bool), resp_status(tlm_response_status), extensions(tlm_extension_base)
        - generic payload: mmap buses, cmd
    + 2. sockets
    + 3. generic payload (cmd, address, byte enables, response status), extensions
    + 4. base protocol (BEGIN_REQ, END_REQ, BEGIN_RESP, END_RESP)
  - Utilities, Convenience sockets, Quantum keeper (LT), Payload event queues?
  - "Towards a Unified Solution for SystemC Tracing and Analysis, SystemC Evolution Fika September 2023"
    TODO
* tlm 2.0 example COSIDE
  THE DESIGN ENVIRONMENT FOR HETEROGENEOUS SYSTEMS
  SystemC / SystemC AMS based Simulation and Modeling Technologies
  - SystemC-AMS/SPICE (A), Modelica (A), Ptomely, SystemC/TLM, SystemC-AMS(A/D)c
* https://github.com/najaeda/naja Structural Netlist API (and more) for EDA post synthesis flow development
  * scale: should handle multi-millions of gates, potentially billions
* glibc-hwcaps
* uvm (environment depends on proprietary tools), architecture limitations
* cocotb/pyuvm/verilator (incomplete simulation envs, long dev time, requires arch reinvention of tests)
* simulators (icarus, questa) -> perf drop on large circuits
* formal proof -> rarely applicable at large scale
* https://github.com/The-OpenROAD-Project/OpenROAD
* Zero ASIC FGPA company
  * SiliconCompiler github.com/siliconcompiler/logik (open bitstream for vendor compat)
    - OpenRoad, Yosys, Verilator, Slang, Klayout, VTR, GHDL, Xyce, Synopsis, Cadence,
      Mentor "Invited: A Distributed approach to Silicon Compilation"
    - not for selling
    - Platypus: Open standard FGPA with BRAM, DSP github.com/siliconcompiler/logiklib
    - fastest QOR FOSS FGPA synthesis github.com/zeroasiccorp/wildebeest
  * SiP Design
  * Switchboard github.com/zeroasiccorp/switchboard (cloud scale verification)
  * Fennec RV CPU
  * Emulation emulation.zeroasic.com/emulation
  * Efabric
  * Chiplet Links
  * Ebrick
  * Epiphany DSP/NoC
* LibreLane https://github.com/librelane/librelane ASIC infrastructure library
* DRAMSys SystemC/TLM2 library based on DRAMml (formal language based on time petri nets)
  DRAM simulation with the simulator DRAMSys (Matthias Jung)
  - Trace Analyzer
* FlowSpace SRAM compiler
* FABulous eFGPA framework https://github.com/FPGA-Research/FABulous
* GDSFactory MEMs, photonics, quantum, RF => heavily simulation based
  - proprietary GUI, data management, PDKs
  - opencore solvers, plugins, PKDs
* Dyno-SV: IR-driven RTL synthesis tool (system verilog compiler/synthesis tool
    on top of Dyno-IR)
  - starting as LLVM/MLIR/CIRT backend to utilize metalanguages
  - LLVM-style compiler, every level of abstraction representable in IR, incremental lowering
  - MLIR trade-offs for high-levle IRs, MLIR/CIRCT intended, LLVM already slow
  - potential backend for CIRCT
  - Thin 64bit ObjRef<T>, DynObjRef (id, custom, rtti)
  - Fat 128bit FatObjRef<T> (ptr, custom, id), FatDynObjRef (ptr, id, cust, rtti)
    * id 4 byte; rtti 2 byte; custom 2 byte; ptr 8 byte
  - contrast: https://circt.llvm.org/docs/Dialects/Synth/RationaleSynth/
    Majority-Inverter Graph (MIG) "Majority Logic Synthesis" by Amarù et al
* Surfer (waveform and source-level debugging)
* cocotb 2.0 (no compilation, so trade-off of how much insight needed)
  - more TLM means better perf
   (ai) verification tool*   | python + pkgs
   metrics analyzer*         | python + pkgs
   test runner*              | python + pkgs
   scripts                   | python + pkgs
   matlab/c/verification ip  | python + pkgs
   sc/uvm                    | python + pkgs
   dut(HDL)                  | dut(HDL)
   simulator*                | simulator (any vendor/FOSS)
   * by same vendor
  - low perf/scalability, sw-mindset
  - faster testbench bring-up, simulation freedom (co-verification,etc), python
  - highly parallizable for fuzz-testing, cloud workloads etc
* uvvm, osvvm, ovm, accelera uvm
* antmicro/kenning-pipeline-manager, /testplanner, /topwrap IP component combination
* antmicro/uvmdvgen generation of generic and repeatble code for agents,testbenches,envs
* antmicro/coveview generation of coverage dashboards
* SystemRDL/PeakRDL automating generation of register
VLSI design flow deep dive
https://youtu.be/9T53opxVQiY?t=600
* more low level parts
https://youtu.be/dIEc_nPky7U?t=81
* low level optimization parts

====nvm
NVM (non-volatile memory)
* opportunity to rethink the entire system stack
* Twizzler as Data-centric OS for Non-volatile Memory
  - most persistent pointer operations <0.5 ns added latency
  - ops up to 13x faster than Unix, SQLite queries up to ~4x faster than on PMDK
  - YCSB workloads ran ~1-3x faster than on native and NVM-optimized SQLite backends

====bad_designs
(according to Linus)
* big endian (things like PCIe fundamentally little-endian)
* virtually tagged caches (Virtually indexed, virtually tagged (VIVT))
  - cache alias issues, flushing caches on context switches
  => VMSAv7 has IVIPT extension; base ARMv7 uses VIVT; so much fun
* only do aligned memory access
  - bonus when not faulting (loading and storing garbage instead)
  => slow on mips, sparc, xtensa, hexagon, maybe slow on riscv, arm5/6, loongarch;
  need to check which archs do not fault and load/store garbage
* expose pipeline details in ISA
  - delayed branch slots (RISC-V bruh), explicit instruction grouping (Itanium bundles)
* extended memory windows
  - HIGHMEM (DOS)
* register windows (extended memory for register) (SPARC, IA64)
  - filling, spilling (limited) for software dealing with faults (nesting exceptions)
  - bonus when rotating and overflowing silently
  => see RISC-V windowing/register renaming/register rotation experiments
  "Clockhands: Rename-free Instruction Set Architecture for Out-of-order Processors";
  4 hands/register groups handled purely in sw and without the mentioned drawbacks
  need to check if it is also chunked to prevent writes into same
* require sw fallbacks for anything unusual
  - make TLB fills fault to sw impl (not common with like 10-20 instructions)
  - denormals or FP precision problems left to sw
  => not sure about TLB fills faults, f80 has "nice" FP precision problems
* make exceptions asynchronous
  - allow machine check exceptions in any context
  => need to check maybe
  - take non-maskability of NMI to heart to make sure sw cannot write atomic code
  => some DSPs or ARM versions need slow software-emulated atomics
  - make sure special cases not dealt in hw are delayed for extra fun
  => need to check maybe

====benchmarks
https://github.com/philipturner/metal-benchmarks

====physics
Essential Semiconductor Physics by Mark Lundstrom
* Unit 1: Materials properties and doping
* Unit 2: Rudiments of quantum mechanics
* Unit 3: Equilibrium carrier concentrations
* Unit 4: Carrier transport, generation, and recombination
* Unit 5: The semiconductor equations

====ai_hardware
https://semiengineering.com/why-in-memory-computation-is-so-important-for-edge-ai/
https://semiengineering.com/memory-wall-problem-grows-with-llms/
* hardware developments without tangible outcome yet (20251230)

====hdl_synthesis
"Behavioral Synthesis Methodology for HDL-Based Specification and Validation" by D. Knapp, T. Ly, D. MacMillen, R. Miller
https://en.wikipedia.org/wiki/Device_driver_synthesis_and_verification#Device_driver_synthesis
* no open source platform driver synthesis (or synthesis for multiple versions) yet
* systemc/tlm synthesis exists
* "Synthesizing Device Emulators" by Sepehr Noorafshan
* "On Synthesising Linux Kernel Module Components from Coq Formalisations"
  coq based synthesis known, but hdl<->coq relation not existing
* Termite2 exists

====timing
Synthesis: inputs (rtl, sdc, .lib, .lef), logic synthesis (yosys), output files(netlist sdc)
Floorplan: initialization, io placement, timing-driven mixed size placement,
  macro placement, trapcell and wallbe insertion, PDN generation
Placement: global placement with(out) placed IOs, io placement, resizing and buffering,
  detailed placement
CTS: clock time synthesis, timing optimization, filler cell insertion
Routing: global routing, detailed routing
Finishing: metal fil insertion, signoff timing report, gdsii gen(KLayout),
  DRC/LVS check(KLayout)
* more/less impact, less/more accuracy, less/more legalization

====chiplet_standards
* UCIe (used in HPC, has multiple protocols and no mechanical spec)
* BOW, AIB only have eletrical spec
* only HBM has mechanical, eletrical, protocol spec
* UMI as more scalable AXI compatible with UCIe, BOW, HBM
  - PUMI (protocol [ethernet,pci-e]), TUMI (transaction [req/resp]),
    SUMI (signal[packet,validity, ready]), LUMI (link layer)
    + CLINK, BOW, AIB
  - tested with AXI, TileLink
  - github.com/zeroascicorp/umi
  - no cache coherency yet

====architecure
"Architecture 2.0: Toward Open Source Foundation Models and Datasets for Hardware Design"
Technology (Moore,Dennard's scaling, dark silicon)
Architecture(hw-support for parallelism, acceleration)
Optimization(application-tuning, full-stack optimization, hw/sw co-design)
Specification(domain-specific hardware)
Future: AI-assistance/automation
1 no datasets, 2 unclear ML algos, 3 workforce & training(no best practices),
4 tools & infra, 5 best practice
idea: simulators (speed vs accuracy) for data generation
* no tangible first delivery
* design space exploration probably first tangible goal
* goal: reduce cost of building chips

====bus
Datapath (internal CPU/data bus)
  https://en.wikipedia.org/wiki/Datapath
  https://en.wikipedia.org/wiki/Microarchitecture
* pipelined datapath most commonly use
* CPU has mutliple datapaths
* simplest design: common internal bus
* courses teach design via finite-state machine with data path (FSMD)
  - use variables or arithmetic operations/conditions
  - equivalent to a Turing machine in expressiveness
  => "Stacked FSMD: A Power Efficient Micro-Architecture for High Level Synthesis"
  => very inefficient in comparison to propriey EDA tooling vendors offering processor
  description languages + graphical/top-bottom exploration
* Datapath Blueprint shows resulting CPU
* courses
  - Designing a Multicycle RISC-V Processor Datapath on Artix-7 FPGA
  - https://cca.informatik.uni-freiburg.de/riscv-simulator/
  - example with calculations https://www.doc.ic.ac.uk/~wl/teachlocal/arch/HP05/multi-cycle-datapath.pdf
* hazards
  - Structural Hazards
  - Data Hazards
  - Control Hazards
https://stackoverflow.com/questions/63234968/width-of-bus-betwen-cpu-cache-and-cpu

1. Explain a modern internal CPU bus/internal data bus of an out of order CPU + design trade-offs.
* efficient data shuffling with multiple CPU cores (very) hard
  - heavily depends on the state machine (algorithm) and data being processed
  - simpler state machines are rather short loops easily handled by the instruction cache
  - most internal states will be carried by registers
    * code like any other software
* info on architecture functionality hidden sparsely in manuals and reverse engineered,
  for example via benchmarks; patents only provide hints
  - Mill architecture dev: I will explain in detail how this works
    architecturally on the hardware level, but for all of you who already have
    knowledge in the field will know that this is oversimplified to the point
    that I am lying."
  - many CPUs to mask their internal bus timing and randomize it
    (likely to mask which cores are actually disabled on the die)
  - benchmarks to determine disabled cores by measuring latency between cores
    across bus operations (l3 hits/cores on other side of chip)
    * reverse engineering for high perf like HFT

https://stackoverflow.com/questions/35258524/why-arent-out-of-order-cpus-troublesome
* reorder buffer which allows instructions to issue out of order but commit in order
  - interrupt (instruction/page fault) => consistent state where all earlier
    instructions have fully executed, and no effects of later instructions have
    happened (to memory or registers), other than populating caches

System Bus
  * only used in embedded, otherwise too slow
  * Data Bus: actual data
  * Address Bus: points to where data lives
  * Control Bus: manages timing, coordination, direction of signals

https://en.wikipedia.org/wiki/Out-of-order_execution
https://en.wikipedia.org/wiki/Dataflow_architecture
