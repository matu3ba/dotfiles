https://news.ycombinator.com/item?id=4128208
All of these assumptions are wrong
    An UTC minute is 60 seconds.
    There are always 24 hours in a day.
    Months have either 30 or 31 days.
    Years have 365 days.
    February is always 28 days long.
    Any 24-hour period will always begin and end in the same day (or week, or month).
    A week always begins and ends in the same month.
    A week (or a month) always begins and ends in the same year.
    The machine that a program runs on will always be in the GMT time zone.
    Ok, thatâ€™s not true. But at least the time zone in which a program has to run will never change.
    Well, surely there will never be a change to the time zone in which a program hast to run in production.
    The system clock will always be set to the correct local time.
    The system clock will always be set to a time that is not wildly different from the correct local time.
    If the system clock is incorrect, it will at least always be off by a consistent number of seconds.
    The server clock and the client clock will always be set to the same time.
    The server clock and the client clock will always be set to around the same time.
    Ok, but the time on the server clock and time on the client clock would never be different by a matter of decades.
    If the server clock and the client clock are not in synch, they will at least always be out of synch by a consistent number of seconds.
    The server clock and the client clock will use the same time zone.
    The system clock will never be set to a time that is in the distant past or the far future.
    Time has no beginning and no end.
    One minute on the system clock has exactly the same duration as one minute on any other clock
    Ok, but the duration of one minute on the system clock will be pretty close to the duration of one minute on most other clocks.
    Fine, but the duration of one minute on the system clock would never be more than an hour.
    It will never be necessary to set the system time to any value other than the correct local time.
    Ok, testing might require setting the system time to a value other than the correct local time but it will never be necessary to do so in production.
    Time stamps will always be specified in a commonly-understood format like 1339972628 or 133997262837.
    Time stamps will always be specified in the same format.
    Time stamps will always have the same level of precision.
    A time stamp of sufficient precision can safely be considered unique.
    A timestamp represents the time that an event actually occurred.
    Human-readable dates can be specified in universally understood formats such as 05/07/11.

Some more falsehoods:

1. Time never goes backwards (as other people have pointed out, time zones break this).
2. UTC time does not go backwards: Leap seconds are implemented as a minute with 61 seconds.
   * The value returned by POSIX time(3) can go backwards (with sub-second precision).
3. The system boot time never changes. On most platforms, the current time is defined as "boot time plus uptime", and setting the current time is performed by changing the boot time.
4. System uptime never goes backwards. Some platforms handle setting the current time by changing the system uptime.
5. POSIX's CLOCK_MONOTONIC never goes backwards. On some platforms and virtualization environments this can break with CPUs shared between virtual machines.
6. On systems without virtualization, CLOCK_MONOTONIC never goes backwards. On some platforms this can occur due to clock skew between CPUs.

tldr;
- use ISO 8601 TZ, because there is no more compact alternative to TAI yet
- bear in mind that 'unix time' can 1. be simple uptick clock, 2. synchronized with utc or 3. be configured as TAI
- check for TAI configuration on the system, see https://en.wikipedia.org/wiki/Unix_time#Variant_that_counts_leap_seconds
- consider use of CLOCK_BOOTTIME
- time steps are increased and decreased to prevent introduction of leaps
- leap seconds are discrete jumps, so strictly speaking one needs to record 1. timestamp and 2. known leaps to
ensure that leap seconds are correctly taken into account, if one uses any string interpretation
- 'Precision Time Protocol' is used to synchronize timestamps to 'International Atomic Time' or 'Coordinated Universal Time'
- 'Terrestrial Time' is the idealization used for 'International Atomic Time'
- time is relative, so all time data must be seen in the appropriate context

There is no portable api to check Kernel time configurations, so if needed you need
to 1. setup your own Kernels xor 2. provide your own time synchronization network
and accept + handle offline inaccuracies and leap seconds yourself.

UTC timestamp -> unix time conversion for size and speed from
https://blog.reverberate.org/2020/05/12/optimizing-date-algorithms.html