====theory
====verification_techniques
====tools
====verification_costs
====idioms
====weak_memory
====formal_problems
====optimality
====state_of_art
====correctness_implementation
====interfaces
and misc

====theory
* synchronization (XMM general model, XC20 model for C as candidates)
* Integer arithmetic, Modular arithmetic, Saturation arithmetic
* Floating point arithmetic
* Separation logic as pointer theory (to optimize pointer accesses)
  - can/should be extended with synchronization model
* memory logics for functional+time behavior of cache, RAM and other storage?
  * state automata known for synchronization of cache lines, but the rest?
* SIMD logic?, Ralf Jung's group had a paper for common SIMD things
* segmentation of depedendencies for ASIL/safety
* "Temporal Stream Logic: Synthesis beyond the Bools"

List of dedicated formally proved languages
- Simulink idea explanation
- idea message passing languages
- idea automata verification
  * standard formats for automata
  * import
  * codegen
    + PLC has codegen from ladder diagrams and other graphical methods
- P language based on actor model (without message loss)

Synthesis
* Temporal synthesis hard problem
* Synthesis competition www.syntcomp.org
* Synthesis from Logical Specifications
  - LTL
    + ltl  ->  non-det. Büchi automaton ->  determ parity autom. -> parity game          ->
       #states: exp     #states:doubly_exp,#colors:exp  #states:doubly_exp,#colors:exp    game solving:poly-states,<exp in colors
      => ltl synthesis 2 EPXTIME-complete
    + same argument for CTL* without det. and non-det. automaton
  - CTL
    + CTL -> tree automaton EXPTIME-complete
    + hard to specify
  - General Reactivity(1)/GR(1)
    + Assumptions A1 ^ A2 .. -> G1 ^ G2 ..
      as state formulas, safety property G (phi -> X psi), liveness properties (G F phi)
    + EXPTIME-complete
    + problem: specification can not be expressed directly in GR(1)
      => pre-synthesis construction of determ. monitor doubly_exp
* Bounded Synthesis
  - fast finding of simple solutions ("is there impl with no more than N states" instead of "is there impl sat spec)
  - LTL
    + synth vs bounded synth: 2 EXPTIME (in len of fomula as input) vs NP-complete/EXPTIME (in size of impl as output)
    + ltl  ->  universal co-Büchi automaton -> constraint system    ->
       transl:exp(in formula)           transl:poly     constraint_solving:NP(in bound)
      => Bounded LTL synthesis NP-complete in size of impl
    + universal co-Büchi automaton accepts a word iff all runs satisfy the co-Büchi condition.
    + solution representation: basic SAT, input-symbolic QBF, state-symbolic DQBF, fully-symbolic DQBP
    + solution strategies
      * increase bound: implementation found, counterstrategy found
      * termination: there is a impl or counterstrategy with at most doubly_exp many states
  - Bounded cycle synthesis
    + additionally minimizes the number of simple cycles.
    + needs additional parameters
    + #cycles of FSM EXP-bounded in size of FSM
    + realizable LTL formula phi, st every impl has at least triply-EXP many cycles in size of phi
* Distributed Synthesis
  - multiple distributed components
  - decidable: closed systems, open systems, pipelines, rings, weakly-ordered architectures
  - undecidable: independent processes, architectures with information forks
  - unclear: decidability of causal memory synthesis
    + in general verifiable for shared memory access in SW PLs, although details for LLVM, C etc are hashed out
    + see "What’s Decidable About Causally Consistent Shared Memory?"

====computing_models
* temporal logic (LTL, CTL, TLA+, PSL, SVA)
* separation logic
* first order logic
* reachability logic
* (relational numerical) abstract domains (ellipoids, intervals, etc)
* various more specification languages

====verification_techniques
* model checking (systematic+exhaustive mathematical model exploration)
  * symbolic execution
    - input-relation based execution
  * bounded model checking
    - for example bounded by number of threads, loop expansions, steps
  * abstraction
    - for example abstract interpretation
  * refinement
    - for example counter-example guided refinement
* deductive verification (solving proof obligations)
  * proof assistants
    - for example Isabelle
  * theorem provers
    - for example z3


====tools
* https://dagitty.net/
* venn.nvim
* Gephi
* SysML (extension of UML): Eclipse Capella https://mbse-capella.org https://github.com/eclipse-capella/capella
* EPK, BPMN
* Alloy (consistency)

====verification_costs
Proof effort is quadratic to code size, if code is not side-effect free.
Side-effect free means the following holds:
* 1. same address space
* 2. no interaction between execution contexts (processes, threads etc)
* 3. no IO
* 4. ignoring access time of memory/cache
More intuitively: Being free of side-effect means relevant constrains can be
derived from the code, which allows automatic proof code generation.
This also means that, in theory, a sufficient powerful model can also explain
code in a side-effect free manner by capturing all allowed semantics.
idea paper as source

====idioms
https://buttondown.email/hillelwayne/archive/nondeterminism-in-formal-specification
* nondeterminism idea
* Idris2 dependent types alone are too low-level.
* Zig is a systems language, which solves biformity from different compile-time
  and runtime semantics for most parts.
* High-level code to simplify problem to simplify memory, strings, atomics not
  possible in Zig
* unclear mapping of CSP/parallelism to actor model ie used by https://github.com/p-org/P

====weak_memory
* best theory overview and consequences "The One-Decade Task: Putting std::atomic in CUDA. - Olivier Giroux - CppCon 2019"
  https://www.youtube.com/watch?v=VogqOscJYvk
* only promise theory covers all semantics, but no tooling exists
* "Multicore Semantics: Making Sense of Relaxed Memory"
* C/C++
  - sequential
  - concurrent with locks
  - with seq_cst atomics
  - with release and acquire
  - with relaxed, fences and the rest
  - with all of the above plus consume
* Thin Air Problem big issue on weak memory to reason about code.
  - Basic issue: compiler analysis and optimisation passes examine and act on the
    program text, incorporating information from multiple executions
* Linux with own model "believed/hoped to be preserved by compilers"
* Conclusion
  - HW models just work, but nothing fully integrated with ISA+concurrency in provers
  - System models (hypervisors, Kernels) ongoing work
  - PL models with C/C++ not bad, but thin air problem big problem for reasoning
    about code (in weak memory context) using relaxed atomics in arbitrary ways
    * no clear path forward wrt tradeoffs
* "Using LKMM atomics in Rust" on lwn.net
  - https://lwn.net/Articles/993785/ TODO atomics semantics

====formal_problems
- 1. implicit proof-carrying code
- 2. explicit proof-carrying code
- 3. verifying compilers with its cost
  * 2 general usable and fully verified optimizing compiler implementations
     + compcert
     + pancake (successor to cogent in sel4 project)
       o lowers to Isabelle or other proof systems or binary code as compiler backends
  * verifying the abstract machine the compiler works on
     + https://github.com/ziglang/zig-spec/issues/40
     + Rust
     + LLVM slow moving target with pointer semantics model very unfinished
       o CHERI as demanding target still unfinished
       o unclear what IO and programmer control over optimizations of pointers necessary,
         so implementors conservative on decisions
       o see "pointer semantics"
     + there appears to be no C model due to being underspecified
     + C++ is probably too convoluted to build a formal model
- 4. Incremental verification via accurate tracking of code changes in editor
  * research in this area tries to recompute how git commits have changed
    code instead of eliminating the problem at the root
  * idea explain
- 5. homotopy type theory as dependent types
  * unclear practicality outside of proof systems (https://www8.cs.fau.de/teaching/hott/)
    https://homotopytypetheory.org/book/
    https://softwareengineering.stackexchange.com/questions/262377/what-are-the-practical-implications-of-homotopy-type-theory-in-programming
    https://discuss.ocaml.org/t/what-sort-of-mathematical-foundations-are-required-to-contribute-to-the-research-on-modular-implicits-typed-effects-and-the-like/12926/2
    + unclear relation of koka on website https://koka-lang.github.io/koka/doc/book.html
    + apparently paper explains some more, but unclear why they are unable to explain in simple words
      the essence
    + koka language
      o 1. effect types sound painful to annotate exceptions are a huge antipattern
      o 2. "deep safety guarantees backed by well-studied category theory"
      without quote to explain sounds bogus,
      o 3. Perceus Optimized Reference Counting "With Perceus, we hope to cross
      this gap and our goal is to be within 2x of the performance of C/C++.
      Initial benchmarks are encouraging and show Koka to be close to C
      performance on various memory intensive benchmarks. " should explain
      class of supported programs and how bindings work for the unsupported
      ones (due to reference counting being too slow).
      o personal: unless they have a equivalence proof for imperative and
      functional classes of program semantics, I'd not trust them that perf
      will be identical through "magical optimizations"
      > you can read through the ACM-pdfs. there is needed info
      > 2 If you know HoTT or if you would take a look, u'll get a better idea what stands behinds this
      > 3 you can read about this in ACM-pdfs too, it's a neat thing, and it works, and is fast (not blazingly, but fast enough).
      when libuv will be inside koka one could solve almost (if not any) class of problems
    + logical equivalence and deduction is very powerful, but to me only
    applicable for semantic correctness; not code optimizations.
      o optimization = equivalence/deduction + cost model
    + pending response how practical useful type theory is
    + >theoretical compile perf/scalability
    structural solution is to ship the schedule / ship the proof
- 6. atomic memory reordering semantics
  see templates/formal_modeling/weak_memory
- 7. pointer semantics is based on Separation logic (extension of Hoare Type
     Theory with associated aliases)
   * instead of "update the value", "update the value for all associated
     aliases"
   * https://github.com/rust-lang/rust/issues/129010 address 64 bit leaves out
     question on who can use what bits of a ptr
   * top byte is not part of the address on aarch64, x86_64; compilers rely on it though for optimizations
   * pointer provenance semantics for provenance based aliasing analysis and optimizations
     + "Zapping pointers out of thin air" on lwn.net https://lwn.net/Articles/993484/
       very informative article and discussions regarding 1. atomics, 2. pointer semantics
       o TODO discussion points
       article tldr;
       - 1. ABA tolerance requires to annotate when pointer is physical instead of logical
       (and "angelic provenance" to simplify stuff for devs probably at compilation time cost + memory doesnt help with that)
       - 2. writes are atemporal relations/links, reads create temporal relations as can be shown on real machines
       - 3. pray that compiler devs fix their semantics on OOTA stuff and dont create additional loads
       and theres stuff for compiler folks + link to simple looking model to eliminate OOTA (although still incomplete/unfinished) where Java, C etc commitees gave up.
     + Rust
       o https://rust-lang.github.io/rfcs/3559-rust-has-provenance.html
       o explicit management of provenance by programemrs
       o plan how to move forward: experiment, incrementally adjust
     + C
       o https://www.open-std.org/jtc1/sc22/wg14/www/docs/n3005.pdf
       o 1.3.4 Pointer exposure and synthesis okish
       o terminology around optimizations lacking
       o terminology: "type-based alias analysis" vs "provenance based alias analysis" etc
       o very much implementation defined behavior and unspecified things
       o no definitive plan or path forward
     + CHERI C
       o https://www.cl.cam.ac.uk/~pes20/asplos24spring-paper110.pdf
       o acknowledges to be very incomplete
       o no definitive plan or path forward
     + LLVM     nothing
       o https://rust-lang.github.io/rfcs/3559-rust-has-provenance.html#drawbacks
       o buggy and no plan how to move forward
     + gcc      nothing
       o https://rust-lang.github.io/rfcs/3559-rust-has-provenance.html#drawbacks
       o buggy and no plan how to move forward
   * TODO https://fil-c.org/ https://github.com/pizlonator/fil-c/ model + perf numbers
   * CHERI example usage (mixed tagged pointer capabilities)
     one implementation may choose
     | perms | padding | ot    | bounds |
     | 6 bit | 1 bit   | 3 bit | 22 bit |
     |              address             |
     |              32 bit              |
     perms: compressed permissions field
     ot: object type
     bounds: compressed base and top bounds
     with semantics as specified in "Capability Hardware Enhanced RISC
     Instructions: CHERI Instruction-Set Architecture (Version 9)"
   * CHERI C model is very incomplete
     "The discussion here of the interaction between CHERI hardware architectural guarantees and C compiler optimisations
     and undefined behaviour makes clear that further work is needed to understand what precise security properties
     CHERI C implementations could reasonably provide."
   * Temporal safety for 2% perf cost looks sweet https://www.cl.cam.ac.uk/research/security/ctsrd/pdfs/2020oakland-cornucopia.pdf
     * unfortunately might blur semantics wrt temporal safety and could make them dependent on the platform under
     3.11 Capabilities and provenance "(3) Checking whether a pointer refers to a live allocation."
   * CheriBSD has experimental userspace temporal memory safety,
     see https://www.morello-project.org/cheri-feature-matrix/ and https://ctsrd-cheri.github.io/cheribsd-getting-started/features/temporal.html
   * Verification-oriented model for Integer-Pointer cast VIP more powerful
     and does not mandate creation of unused invalid pointer being Undefined Behavior(UB)
   * A Provenance-aware Memory Object Model for C (PNVI-ae-udi, PNVI-ae,
     PNVI-plain, and PVI semantics for C) does not cover all the edge cases from pointers
     https://www.cl.cam.ac.uk/~pes20/cerberus/clarifying-provenance-v4.html#q3.-can-one-make-a-usable-pointer-via-casts-to-intptr_t-and-back
- 8. build system parallelization relies on "Jobserver Implementation" protocol
   * only reliable behavior is to close file descriptors and related process owned resources on process termination
   * Pain point: protocol relies on a process manually releasing resources
     before it dies, rather than relying on kernel-managed resources that are
     automatically released when the process dies
   * https://gittup.org/tup/build_system_rules_and_algorithms.pdf "Build System Rules and Algorithms" by Mike Shal
   * https://make.mad-scientist.net/papers/jobserver-implementation/
   * token write to child and read after finishing, if incoming data exists
   * token may get lost (S->P->C with C dying and P not providing info to S)
     and algorithm has no time bound to detect this
   * supervision (library) necessary
   * problem/design space (limitation to globally coordinate (using a local TCP/UNIX socket or something))
     https://stackoverflow.com/questions/20076916/waiting-for-grandchild-after-child-process-has-died
     * fundamental issue is that each process only has a single process group ID. Process groups do not form a tree.
     * Linux cgroups v2
       => https://lwn.net/Articles/855049/
       freezing not atomically, making it atomic would mean it would become slow
       => https://github.com/oconnor663/duct.py/blob/master/gotchas.md#killing-grandchild-processes
     + Linux prctl with PR_SET_CHILD_SUBREAPER
       => https://catern.com/ideas.html
       => http://catern.com/process.html
     + Windows just works mostly, if one gets the signaling correct enough
     + Macos, BSDs etc very unclear likely with same problem
     + pipe to original process with descendants inheriting write end of pipe:
       closed => read-end readable for EOF
       => most tractable for cooperating parent and child processes
     + ptrace
     + use pid 1 or implement pid 1 functionality
     => pid does the same with cgroups v2, but from higher privileges to being permitted by kernel
     which is less racy than kill by pid, but still racy
     + 'pid_t pid = waitpid(-1, &status, WNOHANG)' just works on non-evil children,
       but might just hang
   * can not track processes killed via SIGKILL (especially grandchildren),
   direct children work better via pidfd_send_signal
   * Solution space https://monorepo.tools
   * Proposed Solution to Job server implementation:
     + no (portable) solution exists to fix the fundamental problems of Make Jobserver protocol, see https://github.com/rust-lang/cargo/issues/14102
     + jobserver protocol and non-crashing threads or processes should be faster, see https://github.com/ziglang/zig/issues/20274
 - 9. debug system
    * rr simulates threading and multiple processes in one process to get
    accurate performance counter timings for accurate reproducibility.
    This also rules out searching for tricky race conditions between threads.
    * see ./templates/debug_ideal and prefixes with debug_
- 10. Very good high level reasoning on ISAs ("How to Design an ISA")
  https://news.ycombinator.com/item?id=39031555
    * Nobody bothers documenting ISA including time information ?except Agner Fog?
    * Outside of embedded/security domains, there won't be a close-to-optimal
      ISA for high perf (1. big core) in the near future 2. unless completely
      new market creates demand
      + Investment costs are too high
      + No sane way to lower without efficient emulation compatibility
      inheriting the very same design flaws.
      + Formal models with explicit time information help, but few economic
      incentive and a lot incentives against beyond simulations
      + smaller ones like RISC5 may be feasible, see sel4 project
    * Learning an optimal ISA from given ones: fixing 2 out of those 3
      (1. time, 2. instruction semantics, 3. source code)
      should work, because one can learn the distribution.
      + requires something like "Multiobjective optimization under uncertainty"
      with big complexity/amount of variables (numbers missing).
- 11. arcan desktop engine semantics
    * more low level description https://arcan-fe.com/2024/11/21/a-deeper-dive-into-the-shmif-ipc-system/
    * UI toolkit
    * provided by OS / DE
    * provided via IPC ABI rather than C/C++ dynamic linking API?
    * security for isolation of graphical components
    * time semantics for the render loop via Xorg compatible protocol?
    * idea formal model possible?
    * idea xorg design
    * https://news.ycombinator.com/item?id=40767108
    * https://trace.yshui.dev/2024-06-xorg-bug.html
- 12. formal model for OS security and scheduling
  * security: capability object model for (dynamic) security permissions vs ACLs for (racy to modify) static ones
  * scheduling: still incomplete wrt timing in sel4
  * Assuming a process Pa with schedule A with deadline a and security level 2.
  * Is affecting the scheduler a security problem, because security incidents must be
    processed imminently or with what deadlines?
  * What happens, if two processes affect dynamically any security boundaries?
  * Any form of async exceptions or signal handling (ie in Linux) kills hard deadlines.
  * At least in sel4 all properties are static with statically predefined boundaries only allowed
    for scheduling and all bets are off for abstractions on hard deadline (from what the talks suggested).
    See "Verified Protection Model of the seL4 Microkernel" by Elkaduwe et al.
    * how managing the security manager should work?
  * Indications:
    + must keep CIA (confidentialy, integrity, availability)
    + security is a static system property
    + only allowed via delegating sufficient conditioned process groups
    + caller must handle (stacking) signals (if its not a simple mask) of
    child processes (or delegated higher security process), if it would be
    allowed at all up to runtime limit.
  * Ideas to remain time bounded:
    + static specified CHERI C delegation server to delegate unforgeable memory regions
      o would sidestep authorization race condition, if exactly one execution
      unit process (thread or process) can access relevant resources
      o unclear signal handling restriction, options
        - minimal number of global signals essential to ensure program consistency
        (inspired by Windows)
          * SIGINT as user input to cancel/interrupt process ASAP
          * access and memory errors to do debugging things ASAP
        - infinite signal stacking problem
          * too weak interrupt semantics do not allow handling when to block and unblock handlers
            + reasons to interrupt an interrupt? other than debugging?
              o low cost runtime logging of errors with possible execution trace + environment
            + reasons to per thread handle of interrupts vs doing everything in main/interrupt thread?
              o per thread cleanup of resources via refiring of interrupt and
              ignoring in main thread and/or blocking in mission critical never
              to be interrupted (verified) thread doing heavy work
            + reasons to ASAP handle interrupt other than debugging things?
              o system introspection at time-critical events
            + options for internal time-critical signal handling
              o 1.1 automatically block signal -> can not detect signal during signal errors
              o 1.2 dont automatically block signal -> racy
              o 1.3 signal during signal error/signal -> signals have payload vs duplicate signal number
              o 2.1 one thread handles signals -> threads + metadata forced to be
              globally handled instead of only variable for selection "next
              thread"
                * Could this be a problem in practice?
              o 2.2 all threads handle signals, but can mask it -> unclear cause
              effect relations, very likely to have signal during signal situations
        - immediate termination SIGKILL handling for process snapshots?
          * SIGKILL would be better split into Kernel API with something like
            "HARDSIGSTOP", "HARDSIGDUMP", "HARDSIGCLEAN" to dump process memory,
            if sufficient permissions exist
        - kernel backed buffers + mask with access for high perf demands?
          * unclear if per thread or not
          * without combined syscall semantics more latency?
            + signal range thread registered thread handles signals
            + current signal handling thread could set other thread to handle signals, but only
            if thread is signal handling thread
            + process properties like signaling and resource limitations
            process global, so it makes sense to have Kernel based access
            delegation system for process resources
            + counter arguments
              o no "thread-private resources"
              o more complex handling of errors like "capability in thread lost
                due to thread termination"
              o faster access to Kernel things?
              o main thread implicitly needed to keep things simple currently
              not the case?
- 13. Continuous Integration semantics (running untrusted/less trusted processes)
  * 1. process tracking
    + Brief and short: "The Unix process API is unreliable and unsafe"
      https://catern.com/process.html
    + non-cooperating
      o must remove 'setpgid', 'setsid' permissions from child processes to
      keep all child processes in same process group to be signaled ie with
      SIGKILL
      o requires Kernel support (pledge, seccomp bpf, cgroups, namespaces ..)
      o cgroups designed for exactly this use case, ie freeze process group,
      kill it (atomically) and unfreeze it
      https://security.stackexchange.com/questions/168452/how-is-sandboxing-implemented
      o best behavior with {SET,GET}_CHILD_SUBREAPER
    + cooperating
      o signals (SIGCHLD) may get merged
      o 'waitpid(-1, &status, WNOHANG)'
      o https://cboard.cprogramming.com/linux-programming/155448-sigchld-handler-not-executing-dependably.html
      o https://www.gnu.org/software/libc/manual/html_node/Merged-Signals.html
  * 2. filesystem restrictions
     o idea
- 14. general formal model for graphics unfeasible
  * timing in graphics best effort: "lest things go kaboom almost immediately"
  * display server do to render jobs what the printer spooler did to print jobs
  * Problem statement: Multiple programs try to send documents to the printer
    at the same time. How do stop them from messing up the contents of the pages or things coming
    out interleaved or in the wrong order?
  * problematic timing input for deadlines: exceptions/signal handlers,
    rescheduling of processes and security context changes
- 15. filesystem semantics
  * There appears to be no good one handling all errors correctly
    https://danluu.com/file-consistency/, https://danluu.com/filesystem-errors/
  * https://michael.orlitzky.com/articles/problems_with_posix_acls_and_common_utilities.xhtml
    https://michael.orlitzky.com/articles/fix_busted_acls_faster_with_libadacl.xhtml
    idea summary
  * embedded
    + prefer storing in-memory, if possible
    + LittleFS (memory efficient, reliable, speed, metadata)
    + SPIFFS (simple, compat, low mem overhead, efficient)
- 16. secure speculative execution
  * "A Formal Approach to Secure Speculation" by Cheang et al.
  * idea hackernews comment
  * can be simulated, but very hard to abstract/refine
- 17. stateful machine learning
  * kv cache etc
  * idea: spikes are like synaptic connection strengths and represent output of neuron
  * spikes express a sequence over time
- 18. TLA+ as only feasibe system to formally verify non-deterministic
  parallelism, states and changes over time via temporal logic
  * https://www.learntla.com/
  * TODO
  * https://softwareengineering.stackexchange.com/questions/270674/which-language-has-most-advanced-support-for-proof-based-programming
  * actor based languages for distributed systems simplify implementation details significantly
    o P language to apply formal methods https://github.com/p-org/P
    o https://en.wikipedia.org/wiki/Actor_model
  * alternative should be based on CSP todo long form
- 19. Reversible computation
  * Standard concept in computing: linear/affine types, algebraic effect with handlers, cleanups etc
  * For Real Hardware
    + CPU schedulers are hardware-based and thus relative simple. You can think
    of it as an hardware implementation of instruction interpreter programmers
    try to reason about, because programmers can not affect the exact schedule
    of instructions. In contrast to that GPUs (and related designs like TPUs or
    NPUs) tend to provide more direct ways of control over schedules to
    optimize performance for the relevant use cases.
    + Typical CPUs with schedulers don't provide that
      o "too many unsupported instructions or instructions with side effects
      not modeled properly"
      o Needs "a runtime for reversing ops"
  * For Developing (hardware/software)
    + paper "Exploiting Reversible Computing for Verification" by Burgholzer and Wille
    + idea: Use libraries to design with reversible operations for eliminating bugs
    + Hardware Development Techniques
      o reversible gates
      o make fns reversible
      o use and make explicit garbage outputs
      o purely reversible
- 20. fuzzers or fuzzer modes for testing spatial and temporal coverage?
  * must be able to express what to change
- 21. https://github.com/tigerbeetle/tigerbeetle/blob/main/docs/TIGER_STYLE.md
  * theory/paper on bounded vs unbounded reasoning for computer systems?
  * think about design tradeoffs
- 22. commandment of a good file format from "Funky File Formats" by Albertini
  1. magic at offset zero (fast identification without bypass)
  2. clear chucnk structure (forward compat easy parsing/cleanup)
  3. version number (forward thinking)
  4. no duplicity (eliminating discrepancy)
  5. no "constant" variables (hardcoding instead of ossification)
  6. up to date specs (reflect reality)
  7. sample set (theory not enough)
  8. extensibility (format will evolve)
  9. keep the spirit (no reuse for different intent without distinction)
  10. perfect is the enemy of good (avoid over-complexity)
  * examples for good and simple formats
    o quite okay audio
    o quite okay image
- 23. model for coroutine cancellation unfeasible without constrains
  * conditions for cancellation safety can be arbitrary complex
  * "cancellation token"
  * should be implemented on a per-task basis
  * tricky to optimize and error prone
  testing via
    struct Work:
      task: Task = .{ .callback = callback }
      cancelled: Atomic(bool) = .{ .raw = false }
      callback(task):
        self = fieldParentPtr("task", task)
        if self.cancelled.load(): return
        // actual work
- 24. robust reachability as "perfect reproducibility" of violations controlled by attacker
  * Inference of Robust Reachability Constraints by Sellami et al.
  * paper works via satisfiability checking; looks fairly minimal
  * taint analysis does this on input data, might be possible to do coverage based fuzzing; however not minimal
- 25. static analysis to prove bug-free programs
  * https://github.com/codex-semantics-library/codex not usable yet
  * https://github.com/binsec/binsec unclear trade-offs of symbolic execution
- 26. definitions of memory safety is omitting wanted use cases, which renders it semi-useless
  * good links for tooling https://memorysafety.openssf.org/memory-safety-continuum/
- 27. discussion with goodchild on (fundamental) compiler shortcomings regarding synchronization primitives and core idea of
  expressing Consistency Models without constrains/relations based on graphical input/call-site information
  Q: naive questions:
  are atomics just a hack for expressing memory relations, which are just graphs?
  what would a (fancy) graph annotation system bring, which could be stuffed into whatever model solver one can think of?
  at least my naive understanding is that expressing load and store without the intend where it should be applied to is very wasteful on logical correctness, readability and compiler performance.
  A: now you are getting enlightened!  the first thing is that everything you wrote in this message applies to everything in modern pls, not just load and store
  another thing to observe is: suppose you have a program written using weakly ordered memory ops.  and then you go in and replace them all with seqcst ordering.  the externally visible behaviour of the program shouldn't change.  which means the compiler would be totally within its rights to go back in and change all your memory orderings back to weak orderings
  (hop, skip, and a jump from here to full-on transactions, which i think is ideal)
  all of this business with atomics is compensating for the fact that extant compilers and cpus are stupid and can't actually reason about concurrent algorithms (cpus will never be able to; compilers maybe).  so don't try to let them do that, but give them something with some local properties that can be reasoned about a little bit locally
  'stuffed into whatever model solver one can think of'  state-space explodes very quickly, formal reasoning about concurrent algorithms is v hard without lots of explicit guiding.  https://www.youtube.com/watch?v=VogqOscJYvk  not exactly the same, but giroux talks about something similar here, enumerating all the executions with up to n events, where n caps out very low
  flip side of this though is actually maybe you actually want to have nondeterminism in externally visible behaviour.  classic example is serialising a hash table; you probably don't care what order the keys go in.  in that case you don't want to overconstrain that nondeterminism.  so that might be closer to an application of what you're talking about, i'm not sure what the best way to express that would be
  definitely declarative is good
  A2: Maybe a dumb primitive is "good enough" and external solver/bespoke tool can provide reordering information to compiler
  after user has fuzzed etc memory reordings for desired use cases
- 28. Local system bus/IPC interfaces/system challenges
  * 1. CIA
  * 2. (re-)identification
  * 3. performance requirements (latency, throughput, bandwidth)
  * 4. semi-reliable systems schedule with sane error behavior
  * 5. debugging concept (simulation, reversible computing, recording, tracing, stepping, validation etc)
- 29. Distributed system bus/interface system challenges
  same as 28, but with typical distributed properties on top
  * 1 idempotency
  * 2 consistency
  * 3 backpressure
  * (4 observability)
  * 5 fault-tolerance
  * (6 service discovery)
  * (less often)
  * 7 consensus
  * 8 replication

To me unclear if solved:
* check if linker speak has a formal model
  - https://www.cl.cam.ac.uk/~pes20/projects-2024.html
    looks like projects on DWARF debug info tests and linker speak
    tests (or oracle for correct semantics).
* check, if cakeml and compcert have linker speak verified
  - no
* As I understand it, LTO collects "live symbols" and strips the rest besides
doing allowed bitcode optimizations (for strong LTO), but for example the LLVM
docs do not clarify how "symbol is live" is detected
https://www.llvm.org/docs/LinkTimeOptimization.html

====optimality
Theoretical optimal language:
```
Consider a language being bootstrapped (in multiple steps) from assembly.
Such language needs to have a freestanding build, link and dependency resolving
tool to impregnate every system for multiplying the fragments.
1.    can not build+link on the target: broken.
2.    can not resolve to find the ingredients: broken.
3.    too slow for the job: broken.
4.    too undefined: broken.
5.    no hotswapping and REPL for debugging: broken
6.    no usable async: broken
7.    no formal correctness proof of spec and parts: unfinished
Theoretical optimal proof language
1. compositional semantics
2. unbounded loops
3. intuitive proof rules
4. concurrency

One possible strategy:
1.    Have super simple to use + expressive + portable language
2.    Fix build scripts (simple to use + expressive)
3.    Fix dependency tracking+resolving (simple to use + expressive) => come up
      with language agnostic standard
4.    Have a sufficient user base to be reason to switch away from C codegen
      for static analysis + debugging.
5.    Define proper extensions on code plus fancy static analysis tools (borrow
      checking with comment syntax and separate AST/IR, so one has the slow borrow
      checker due to bigger AST and the fast compiler).
6.    Formal analysis interface to compiler (API to extract ASTs + PIE for analysis).
7.    World domination. Build a temple or something.

====state_of_art
- (Partially) verifiable are static and more simple language like 'Structured Text',
  'CompCert C' up to pure functional languages (io restricted) like TODO
- Hotswapping and/or with REPL are interpreted languages like Julia, Python, R
  and all functional + logic Programming Languages
- Static languages intended for optimizing compilers like Rust, C, C++, D
- Wrapup of midori as operating system for reduction of errors (without taking
  into account performance)
  https://joeduffyblog.com/2016/02/07/the-error-model/
- "Program logics à la carte" by Vistrup et al. to compose program logics and
  make use of language-agnostics program logic to build language-specific program
  logic (mixed demonic/angelic choice)
  * TODO understand effect systems and their limitations
- ReLinChe as tool for verifying relaxed linearizability via bounded model checking
  * found ordering violation of https://libcds.sourceforge.net/
  * model checking, partial lock for specification
  * verfying MPCs with 7-9 threads with up to 9 calls below 15 minutes
  * most time spent in enumeration (not refinement)
  * see https://en.wikipedia.org/wiki/Linearizability
- "Relaxed Memory Concurrency Re-executed" by Moiseenko et al. with XMM to possibly fix C memory model
  * https://www.youtube.com/watch?v=Jff0pIbj8PM&t=14362s
  * XMM - novel (almost) per-execution axiomatic-operational semantic framework
    for relaxed concurrency with XC20 as instantation of XMM for the C20 memory model
  * 1 No Out of Thin Air, 2 efficient compilation, 3 local program transformations,
  * 4 thread sequentialization is sound, 5 amendable to model checking
  * not good: promising semantics, Wkmo and others because no support for
  thread sequentialization; C20 has OOTA and can not be model checked
  * thread sequentialization is based on graph reconstruction
  * XMM and RC20 have those properties, so has proposed XC20 model
  * XMC as sound model checker implemented on top of GenMC
  * missing: ROCQ mechanization, sequentially-consistent accesses,
  archive completeness of model checking algorithm
- MIXER: DPOR for MSA
  * stateless model checking can not handle mixed size accesses in general
  * factorial state explosion
  * dynamic partial order reduction, explore partial-ordered traces
  (execution graphs as only necessary graphs)
  Multi-Write Revisits to revisit a read, so how to guarantee optimality?
    - tie-breaking like which steps are allowed
    - unique choice for each visit
  * GenMC to verify lockref clients, #threads up to 4 threads with executions
  reaching more than 100_000 for 5 threads
  * factor <2 overhead for non-MSA benchmarks
- Advanced Weakest Precondition Calculi for Probabilistic Programs
  * calculus specifically tailored to the task of reasoning about expected runtimes
  * use cases: randomized algorithms, describing complex probability distributions

"quantified verification" as verification of quantified properties instead of general/basic ones

====correctness_implementation
System implementation details towards correctness:
- 1. Correctness requires to simplify data representation and handling (both
     serialized and unserialized)
- 2. Correctness requires to limit functionality to what is necessary
- 3. Correctness requires to have a system to limit expressivity
- 4. Correctness requires an axiomatic functional model
- 5. Correctness requires a functional proof of all related parts in an abstract model
- 6. Full trust into correctness requires a fully mechanized proof of all code
     under a correct model of hardware for all known hardware assumptions
     * Accepted tradeoff: Hardware failures are not under control of compiler,
       so the user is responsible to take them into account.

Candidates for an optimal language (independent of domain) working on at least 3 points:
- Compcert C is working on frontend verification and semantics of linker speak (3+4+7 solved, 3+5 unclear)
  * requires manual modeling with costly (manual) existential variables solving TODO what are existential variables used for?
- Rust satisfies 3, 4 for safe Rust and is kinda working on 6 (no completion
  model), but it is not simple, so it can not be optimal (no realistic chance
  to prove correctness or to reduce bug count to 0)
- Zig's strategy is currently to working on 1-3 for version1_0, 4 has PRO, RVO but no model)
```

The claim of memory safe software is that the checker helps you and that the
base safety abstraction is reusable enough justifying the additional
compilation time. This argument falls apart on big scale, specific performance
+ lifetime or enforcement of weak coupling, ie during runtime.

the alternative is to use sel4/micro kernel approach of capabilities, but even
those assume you can not access all memory and/or code is trusted.
https://en.wikipedia.org/wiki/Object-capability_model
* sel4 has proofs on worst cast execution time, confidentialy and non-interference
TODO check extent of confidentially proof interaction with measured architectural side-channels on hardware
TODO check model

 making C and C++ pointers more sim-
ilar to true capabilities—that is unforgeable and unguessable

"Path to components" and talks by Luke Wagner
tldr; posix process model fix suggestions via component model layered on top of WebAssembly Core
=> Wyvern for capabilities + parametric linking
1 parametric linking
  1.1 distingish units of code from instances of that code
  1.2 unit of code declares imports that must be supplied when code is instantiated
  1.3 allow units of code to instantiate other units of code by supplying their imports
  => enables mocking, local testing (no dependency injection framework)
  => dependency resolution (without lockfiles at runtime)
  => service chaining (without network stack)
2 high-levle value types
  2.1 value types include strings, lists, records, variants with subtyping
  2.2 make read/write values in/out of memory configurable yet optimizable
  2.3 allow value types to contain opaque handles
  - file descriptor??
3 resource and handle types
  3.1 decouple
  - resources: are non-copyable things with lifetimes
  - handles: values that point to resources
  - functions: take/return handles, operate on resources
  3.2 resources/handles have resource/handles type to specify resource type
  3.3 resource types are runtime type-tags to be imported/created/exported
  => Layer 7 interfaces that avoid sidecar protocol parsing overhead
  => Resources directly usable without manual glue code
  => Virtual platform layering
4 future and stream types
  4.1 calls between instances can share stack, but can also be suspended
  allowing caller to progress
  4.2 future and stream types for passing and returning handles to in-progress
  tasks
  4.3 low-level control-flow protocols for futures and streams that language runtimes
  can integrate with
  => efficient bulk data streaming into/out of wasm linear memory
  => asnyc interfaces directly usable without manual glue code
  => defining and enforcing cross-language "structured concurrency"
5 putting everything together
  => IO with outside world via WASI (logging, config, fs, sockets, http, grpc, mq, pubsub, kv, sql,..)
  WASI
  ---
  Component Model
  ---
  WebAssembly Core
6 Clarifications
  6.1 Distributed computing is out of scope (including partial failure, persistence,
  live upgrade, scalability, availability)
  6.2 Leave these problems to platform or future higher-level specs
7 Worlds
  * are subsets to expose and to work on (when needed)
  * can be imported or exported
  * can do extensions, union, intersections
  * platform vendor supports world, when implementing imports and calls exports
  * component author targets world by calling imports, implementing exports
  world (embedding made easy, SDKs made easy)
  - wit-bindgen
    - host bindings
      - wasmtime to run things
    - guest bindings (what language)
      - compiler to create things
      - wit-component to create component.wasm
8 Virtualization tooling
  * Virtual Platform Layering
  TODO https://youtu.be/phodPLY8zNE?t=1282

"Components, WebAssembly’s Docker’s Moment - Bailey Hayes, Cosmonic" https://www.youtube.com/watch?v=wttN69wciSM
* sounds like capabilities without being explicit on them
* Erlang model as suggestion
TODO summarize capabilities semantically

"Threading the needle with concurrency and parallelism in the Component Model" by Luke Wagner

idea:
does alpha, beta, eta reduction also work for type checking in semantic analysis?

compiler backends important optimizations
- inline, unroll (& vectorize), CSE, DCE, code motion, folding, peepholes
- arithmetic optimizations for all math stuff
- eliminate redundant checks (specially null checks and bounds checks)

- typical OS permissions model: eventual consistency (signaling, no scheduler
  upper bound for security context etc) or possible race conditions. security
  depends on implementation details to be probably correct and sandboxing being
  sufficient. see also PAC theorem for why

Austral as language with linear type system without boilerplate (might become decent DSL)
https://borretti.me/article/introducing-austral
https://austral-lang.org/spec/spec.html#linearity
- cogent uses also a linear type system https://trustworthy.systems/projects/TS/cogent.pml.
Note: There are other more domain-specific fully verified languages, ie for message passing
compiling to C code or for signal processing etc (Simulink).

====interfaces
- 1. dynamic dispatch https://en.wikipedia.org/wiki/Dynamic_dispatch
  * works good enough
  * how the different type is chosen
    o comptime switch case
    o base pointer with selection
    o pointer to implementation
    ..
- 2. bounded parametric polymorphism https://en.wikipedia.org/wiki/Parametric_polymorphism#Bounded_parametric_polymorphism
  * zig thinks static analysis is mostly not needed and running the code is sufficient for testing and satisfying invariants
  * goal(s): composing constrains, meaning writing code to argue on relations and arithmetic
    o constrain1: comptime evaluation order of top level decls must remain arbitrary
    o constrain2: no blow-up of compilation times
  * example1: type a1 can only be a type with defined equality such that it has a defined fn `equal(a1,a1) -> bool`
  * example2: type a1 can only be a type with fancy function f1 such that it has a defined fn `f1(..) -> ..`
  * ida: build.zig allows target based chaining of different comptime queries is_associative(ty1) is_commutative(ty1) on files
  once zig becomes able to do type queries etc
    o unclear use cases of generics:
      - definition checked generics
      - multi-builds
  * idea: comptime declarative fns for type checks using an interface struct { data, &fn_check_on_data }
    o testing in code or build.zig with all target + type instantiations
    o data or functions can not be arbitrarily composed for generic checks
