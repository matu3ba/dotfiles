List of dedicated formally proved languages
- Simulink TODO context
- TODO message passing languages
- TODO automata verification
  * standard formats for automata
  * import
  * codegen
    + PLC has codegen from ladder diagrams and other graphical methods

====verification_costs
Proof effort is quadratic to code size, if code is not side-effect free.
Side-effect free means the following holds:
* 1. same address space
* 2. no interaction between execution contexts (processes, threads etc)
* 3. no IO
* 4. ignoring access time of memory/cache
More intuitively: Being free of side-effect means relevant constrains can be
derived from the code, wich allows automatic proof code generation.
This also means that, in theory, a sufficient powerful model can also explain
code in a side-effect free manner by capturing all allowed semantics.
TODO paper as source

====formal_problems
- 1. implicit proof-carrying code
- 2. explicit proof-carrying code
- 3. verifying compilers with its cost
  * 2 general usable and fully verified optimizing compiler implementations
     + compcert
     + pancake (successor to cogent)
       o lowers to Isabelle or other proof systems or binary code as compiler backends
  * verifying the abstract machine the compiler works on
     + https://github.com/ziglang/zig-spec/issues/40
     + Rust
     + LLVM slow moving target with pointer semantics model very unfinished
       o CHERI as demanding target still unfinished
       o unclear what IO and programmer control over optimizations of pointers necessary,
         so implementors conservative on decisions
       o see "pointer semantics"
     + there appears to be no C model due to being underspecified
     + C++ is probably too convoluted to build a formal model
- 4. Incremental verification via accurate tracking of code changes in editor
  * research in this area tries to recompute how git commits have changed
    code instead of eliminating the problem at the root
  * TODO explain
- 6. atomic memory reordering semantics
  see templates/formal_modeling/weak_memory
- 7. pointer semantics is based on Separation logic (extension of Hoare Type
     Theory with associated aliases)
   * instead of "update the value", "update the value for all associated
     aliases"
   * unsettled in LLVM blocking CHERI usage (mixed tagged pointer capabilities)
   * Verification-oriented model for Integer-Pointer cast VIP more powerful
     and does not mandate creation of unused invalid pointer being Undefined Behavior(UB)
   * A Provenance-aware Memory Object Model for C (NVI-ae-udi, PNVI-ae,
     PNVI-plain, and PVI semantics for C) does not cover all the edge cases from pointers
     https://www.cl.cam.ac.uk/~pes20/cerberus/clarifying-provenance-v4.html#q3.-can-one-make-a-usable-pointer-via-casts-to-intptr_t-and-back
- 8. build system parallelization relies on "Jobserver Implementation" protocol
   * https://gittup.org/tup/build_system_rules_and_algorithms.pdf "Build System Rules and Algorithms" by Mike Shal
   * https://make.mad-scientist.net/papers/jobserver-implementation/
   * token write to child and read after finishing, if incoming data exists
   * token may get lost (S->P->C with C dying and P not providing info to S)
     and algorithm has no time bound to detect this
   * supervision (library) necessary
   * problem/design space (limitation to globally coordinate (using a local TCP/UNIX socket or something))
     https://stackoverflow.com/questions/20076916/waiting-for-grandchild-after-child-process-has-died
     + Linux prctl with PR_SET_CHILD_SUBREAPER
     + pipe to original process with descendants inheriting write end of pipe:
       closed => read-end readable for EOF
     + ptrace
     + use pid 1 or implement pid 1 functionality
     + 'pid_t pid = waitpid(-1, &status, WNOHANG)' just works on non-evil children,
       but might just hang
   * can not track processes killed via SIGKILL
   * Solution space https://monorepo.tools
 - 9. debug system
    * rr simulates threading and multiple processes in one process to get
    accurate performance counter timings for accurate reproducibility.
    This also rules out searching for tricky race conditions between threads.
    * What's your imagination to what time-travel debugging can be?
    * Egg-laying wull milk pig with mind-blowing complexity and scope.
    A from my viewpoint optimal system would be a graphical + text editor with
    shortcuts to serialize and deserialize 1. program control flow state by
    relative conditions to another (in what source file area which thread or
    process is), and 2. program state state conditions (relative or absolute
    program conditions) and ideally would generalize both to 3. arbitrary
    programs [in which for example kernel information like memory usage could
    be queried] for all introspection use cases (debugging,
    memcheck/validators, race condition checkers, tracers) to have seperate
    overlays for debugging to reproduce the state of one program state
    condition within the other tool even under stochastic events.
    * As far as I understand, currently language semantics leak into debuggers
    besides debuggers/systems with repls can not be designed well for third
    party interactivity, which is why gdb and lldb do not expose pretty printer
    as c api or formatting library etc.
    * More ideally, the semantics of the program itself could be reasoned
    about, but this would require compiler support with complete enough
    memory models + simulator with debug api etc on top of that (say Miri) to
    initialize the simulator from arbitrary program state.
    * As of now, we neither have a cross-usable snapshotformat for programs (on
    the same platform) nor tooling to compare program states or traces.
    * As of now, debug system scripting experience is very bad
      + non-portable
      + natvis (must create bindings), python/javascript (no underlying type info)
    * Kernel mode debug traces are not supported (unfeasible to support in general)
- 10. Very good high level reasoning on ISAs ("How to Design an ISA")
  https://news.ycombinator.com/item?id=39031555
    * Nobody bothers documenting ISA including time information ?except Agner Fog?
    * Outside of embedded/security domains, there won't be a close-to-optimal
      ISA for high perf (1. big core) in the near future 2. unless completely
      new market creates demand
      + Investment costs are too high
      + No sane way to lower without efficient emulation compatibility
      inheriting the very same design flaws.
      + Formal models with explicit time information help, but few economic
      incentive and a lot incentives against beyond simulations
      + smaller ones like RISC5 may be feasible, see sel4 project
    * Learning an optimal ISA from given ones: fixing 2 out of those 3
      (1. time, 2. instruction semantics, 3. source code)
      should work, because one can learn the distribution.
      + requires something like "Multiobjective optimization under uncertainty"
      with big complexity/amount of variables (numbers missing).
- 11. arcan desktop engine semantics
    * UI toolkit
    * provided by OS / DE
    * provided via IPC ABI rather than C/C++ dynamic linking API?
    * security for isolation of graphical components
    * time semantics for the render loop via Xorg compatible protocol?
    * TODO formal model possible?
- 12. formal model for OS security and scheduling
    * Assuming a process Pa with schedule A with deadline a and security level 2.
    * Is affecting the scheduler a security problem, because security incidents must be
      processed imminently or with what deadlines?
    * What happens, if two processes affect dynamically any security boundaries?
    * Any form of async exceptions or signal handling (ie in Linux) kills hard deadlines.
    * At least in sel4 all properties are static with statically predefined boundaries only allowed
      for scheduling and all bets are off for abstractions on hard deadline (from what the talks suggested).
      See "Verified Protection Model of the seL4 Microkernel" by Elkaduwe et al.
      * how managing the security manager should work?
    * Indications:
      + must keep CIA (confidentialy, integrity, availability)
      + security is a static system property
      + only allowed via delegating sufficient conditioned process groups
      + caller must handle (stacking) signals (if its not a simple mask) of
      child processes (or delegated higher security process), if it would be
      allowed at all up to runtime limit.
- 13. Continuous Integration semantics (running untrusted/less trusted processes)
   * 1. process tracking
     + Brief and short: "The Unix process API is unreliable and unsafe"
       https://catern.com/process.html
     + non-cooperating
       o must remove 'setpgid', 'setsid' permissions from child processes to
       keep all child processes in same process group to be signaled ie with
       SIGKILL
       o requires Kernel support (pledge, seccomp bpf, cgroups, namespaces ..)
       o cgroups designed for exactly this use case, ie freeze process group,
       kill it (atomically) and unfreeze it
       https://security.stackexchange.com/questions/168452/how-is-sandboxing-implemented
       o best behavior with {SET,GET}_CHILD_SUBREAPER
     + cooperating
       o signals (SIGCHLD) may get merged
       o 'waitpid(-1, &status, WNOHANG)'
       o https://cboard.cprogramming.com/linux-programming/155448-sigchld-handler-not-executing-dependably.html
       o https://www.gnu.org/software/libc/manual/html_node/Merged-Signals.html
   * 2. filesystem restrictions
       o TODO
- 14. general formal model for graphics unfeasible
  * timing in graphics best effort: "lest things go kaboom almost immediately"
  * display server do to render jobs what the printer spooler did to print jobs
  * Problem statement: Multiple programs try to send documents to the printer
    at the same time. How do stop them from messing up the contents of the pages or things coming
    out interleaved or in the wrong order?
  * problematic timing input for deadlines: exceptions/signal handlers,
    rescheduling of processes and security context changes
- 15. filesystem semantics
  * There appears to be no good one handling all errors correctly
    https://danluu.com/file-consistency/, https://danluu.com/filesystem-errors/
- 16. secure speculative execution
  * "A Formal Approach to Secure Speculation" by Cheang et al.
  * TODO hackernews comment

Interesting ideas
* https://catern.com/ideas.html file-descriptor API for processes
* Summary: Non-portable mess.

To me unclear if solved:
- check, if cakeml and compcert have linker speak verified
- check if linker speak has a formal model
- Model for coroutine cancellation

Austral as language with linear type system without boilerplate (might become decent DSL)
https://borretti.me/article/introducing-austral
https://austral-lang.org/spec/spec.html#linearity
- cogent uses also a linear type system https://trustworthy.systems/projects/TS/cogent.pml.
Note: There are other more domain-specific fully verified languages, ie for message passing
compiling to C code or for signal processing etc (Simulink).

Thoughts on a theoretical optimal language:
```
Consider a language being bootstrapped (in multiple steps) from assembly.
Such language needs to have a freestanding build, link and dependency resolving
tool to impregnate every system for multiplying the fragments.
1.    can not build+link on the target: broken.
2.    can not resolve to find the ingredients: broken.
3.    too slow for the job: broken.
4.    too undefined: broken.
5.    no hotswapping and REPL for debugging: broken
6.    no usable async: broken
7.    no formal correctness proof of spec and parts: unfinished

One possible strategy:
1.    Have super simple to use + expressive + portable language
2.    Fix build scripts (simple to use + expressive)
3.    Fix dependency tracking+resolving (simple to use + expressive) => come up
      with language agnostic standard
4.    Have a sufficient user base to be reason to switch away from C codegen
      for static analysis + debugging.
5.    Define proper extensions on code plus fancy static analysis tools (borrow
      checking with comment syntax and separate AST/IR, so one has the slow borrow
      checker due to bigger AST and the fast compiler).
6.    Formal analysis interface to compiler (API to extract ASTs + PIE for analysis).
7.    World domination. Build a temple or something.

State of art:
- (Partially) verifiable are static and more simple language like 'Structured Text',
  'CompCert C' up to pure functional languages (io restricted) like TODO
- Hotswapping and/or with REPL are interpreted languages like Julia, Python, R
  and all functional + logic Programming Languages
- Static languages intended for optimizing compilers like Rust, C, C++, D

System implementation details towards correctness:
- 1. Correctness requires to simplify data representation and handling (both
     serialized and unserialized)
- 2. Correctness requires to limit functionality to what is necessary
- 3. Correctness requires to have a system to limit expressivity
- 4. Correctness requires an axiomatic functional model
- 5. Correctness requires a functional proof of all related parts in an abstract model
- 6. Full trust into correctness requires a fully mechanized proof of all code
     under a correct model of hardware for all known hardware assumptions
     * Accepted tradeoff: Hardware failures are not under control of compiler,
       so the user is responsible to take them into account.

Candidates for an optimal language (independent of domain) working on at least 3 points:
- Compcert C is working on frontend verification and semantics of linker speak (3+4+7 solved, 3+5 unclear)
  * requires manual modeling with costly (manual) existential variables solving
- Rust satisfies 3, 4 for safe Rust and is kinda working on 6 (no completion
  model), but it is not simple, so it can not be optimal (no realistic chance
  to prove correctness or to reduce bug count to 0)
- Zig's strategy is currently to working on 1-3 for version1_0, 4 is missing PRO and RVO)
```

The claim of memory safe software is that the checker helps you and that the
base safety abstraction is reusable enough justifying the additional
compilation time. This argument falls apart on big scale, specific performance
+ lifetime or enforcement of weak coupling, ie during runtime.

the alternative is to use sel4/micro kernel approach of capabilities, but even
those assume you can not access all memory and/or code is trusted.
https://en.wikipedia.org/wiki/Object-capability_model

idea:
does alpha, beta, eta reduction also work for type checking in semantic analysis?

compiler backends important optimizations
- inline, unroll (& vectorize), CSE, DCE, code motion, folding, peepholes
- arithmetic optimizations for all math stuff
- eliminate redundant checks (specially null checks and bounds checks)