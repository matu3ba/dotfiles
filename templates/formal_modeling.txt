List of dedicated formally proved languages
- Simulink idea explanation
- idea message passing languages
- idea automata verification
  * standard formats for automata
  * import
  * codegen
    + PLC has codegen from ladder diagrams and other graphical methods

====tools
* https://dagitty.net/
* venn.nvim
* Gephi

====verification_costs
Proof effort is quadratic to code size, if code is not side-effect free.
Side-effect free means the following holds:
* 1. same address space
* 2. no interaction between execution contexts (processes, threads etc)
* 3. no IO
* 4. ignoring access time of memory/cache
More intuitively: Being free of side-effect means relevant constrains can be
derived from the code, wich allows automatic proof code generation.
This also means that, in theory, a sufficient powerful model can also explain
code in a side-effect free manner by capturing all allowed semantics.
idea paper as source

====idioms
https://buttondown.email/hillelwayne/archive/nondeterminism-in-formal-specification
* nondeterminism idea


====weak_memory
* "Multicore Semantics: Making Sense of Relaxed Memory"
* C/C++
  - sequential
  - concurrent with locks
  - with seq_cst atomics
  - with release and acquire
  - with relaxed, fences and the rest
  - with all of the above plus consume
* Thin Air Problem big issue on weak memory to reason about code.
  - Basic issue: compiler analysis and optimisation passes examine and act on the
    program text, incorporating information from multiple executions
* Linux with own model "believed/hoped to be preserved by compilers"
* Conclusion
  - HW models just work, but nothing fully integrated with ISA+concurrency in provers
  - System models (hypervisors, Kernels) ongoing work
  - PL models with C/C++ not bad, but thin air problem big problem for reasoning
    about code (in weak memory context) using relaxed atomics in arbitrary ways
    * no clear path foward wrt tradeoffs
* "Using LKMM atomics in Rust" on lwn.net
  - https://lwn.net/Articles/993785/ TODO atomics semantics



====formal_problems
- 1. implicit proof-carrying code
- 2. explicit proof-carrying code
- 3. verifying compilers with its cost
  * 2 general usable and fully verified optimizing compiler implementations
     + compcert
     + pancake (successor to cogent)
       o lowers to Isabelle or other proof systems or binary code as compiler backends
  * verifying the abstract machine the compiler works on
     + https://github.com/ziglang/zig-spec/issues/40
     + Rust
     + LLVM slow moving target with pointer semantics model very unfinished
       o CHERI as demanding target still unfinished
       o unclear what IO and programmer control over optimizations of pointers necessary,
         so implementors conservative on decisions
       o see "pointer semantics"
     + there appears to be no C model due to being underspecified
     + C++ is probably too convoluted to build a formal model
- 4. Incremental verification via accurate tracking of code changes in editor
  * research in this area tries to recompute how git commits have changed
    code instead of eliminating the problem at the root
  * idea explain
- 5. homotopy type theory as dependent types
  * unclear practicality outside of proof systems (https://www8.cs.fau.de/teaching/hott/)
    https://homotopytypetheory.org/book/
    https://softwareengineering.stackexchange.com/questions/262377/what-are-the-practical-implications-of-homotopy-type-theory-in-programming
    https://discuss.ocaml.org/t/what-sort-of-mathematical-foundations-are-required-to-contribute-to-the-research-on-modular-implicits-typed-effects-and-the-like/12926/2
    + unclear relation of koka on website https://koka-lang.github.io/koka/doc/book.html
    + apparently paper explains some more, but unclear why they are unable to explain in simple words
      the essense
    + koka language
      o 1. effect types sound painful to annotate exceptions are a huge antipattern
      o 2. "deep safety guarantees backed by well-studied category theory"
      without quote to explain sounds bogous,
      o 3. Perceus Optimized Reference Counting "With Perceus, we hope to cross
      this gap and our goal is to be within 2x of the performance of C/C++.
      Initial benchmarks are encouraging and show Koka to be close to C
      performance on various memory intensive benchmarks. " should explain
      class of supported programs and how bindings work for the unsupported
      ones (due to reference counting being too slow).
      o personal: unless they have a equivalence proof for imperative and
      functional classes of program semantics, I'd not trust them that perf
      will be identical through "magical optimizations"
      > you can read through the ACM-pdfs. there is needed info
      > 2 If you know HoTT or if you would take a look, u'll get a better idea what stands behinds this
      > 3 you can read about this in ACM-pdfs too, it's a neat thing, and it works, and is fast (not blazingly, but fast enough).
      when libuv will be inside koka one could solve almost (if not any) class of problems
    + logical equivalence and deduction is very powerful, but to me only
    applicable for semantic correctness; not code optimizations.
      o optimization = equivalence/deduction + cost model
    + pending response how practical useful type theory is
    + >theoretical compile perf/scalability
    structural solution is to ship the schedule / ship the proof
- 6. atomic memory reordering semantics
  see templates/formal_modeling/weak_memory
- 7. pointer semantics is based on Separation logic (extension of Hoare Type
     Theory with associated aliases)
   * instead of "update the value", "update the value for all associated
     aliases"
   * pointer provenance semantics for provenance based aliasing analysis and optimizations
     + "Zapping pointers out of thin air" on lwn.net https://lwn.net/Articles/993484/
       very informative article and discussions regarding 1. atomics, 2. pointer semantics
       o TODO discussion points
       article tldr;
       - 1. ABA tolerance requires to annotate when pointer is physical instead of logical
       (and "angelic provenance" to simplify stuff for devs probably at compilation time cost + memory doesnt help with that)
       - 2. writes are atemporal relations/links, reads create temporal relations as can be shown on real machines
       - 3. pray that compiler devs fix their semantics on OOTA stuff and dont create additional loads
       and theres stuff for compiler folks + link to simple looking model to eliminate OOTA (although still incomplete/unfinished) where Java, C etc commitees gave up.
     + Rust
       o https://rust-lang.github.io/rfcs/3559-rust-has-provenance.html
       o explicit management of provenance by programemrs
       o plan how to move forward: experiment, incrementally adjust
     + C
       o https://www.open-std.org/jtc1/sc22/wg14/www/docs/n3005.pdf
       o 1.3.4 Pointer exposure and synthesis okish
       o terminology around optimizations lacking
       o terminology: "type-based alias analysis" vs "provenance based alias analysis" etc
       o very much implementation defined behavior and unspecified things
       o no definitive plan or path forward
     + CHERI C
       o https://www.cl.cam.ac.uk/~pes20/asplos24spring-paper110.pdf
       o acknowledges to be very incomplete
       o no definitive plan or path forward
     + LLVM     nothing
       o https://rust-lang.github.io/rfcs/3559-rust-has-provenance.html#drawbacks
       o buggy and no plan how to move forward
     + gcc      nothing
       o https://rust-lang.github.io/rfcs/3559-rust-has-provenance.html#drawbacks
       o buggy and no plan how to move forward
   * CHERI example usage (mixed tagged pointer capabilities)
     one implementation may choose
     | perms | padding | ot    | bounds |
     | 6 bit | 1 bit   | 3 bit | 22 bit |
     |              address             |
     |              32 bit              |
     perms: compressed permissions field
     ot: object type
     bounds: compressed base and top bounds
     with semantics as specified in "Capability Hardware Enhanced RISC
     Instructions: CHERI Instruction-Set Architecture (Version 9)"
   * CHERI C model is very incomplete
     "The discussion here of the interaction between CHERI hardware architectural guarantees and C compiler optimisations
     and undefined behaviour makes clear that further work is needed to understand what precise security properties
     CHERI C implementations could reasonably provide."
   * Temporal safety for 2% perf cost looks sweet https://www.cl.cam.ac.uk/research/security/ctsrd/pdfs/2020oakland-cornucopia.pdf
     * unfortunately might blur semantics wrt temporal safety and could make them dependent on the platform under
     3.11 Capabilities and provenance "(3) Checking whether a pointer refers to a live allocation."
   * CheriBSD has experimental userspace temporal memory safety,
     see https://www.morello-project.org/cheri-feature-matrix/ and https://ctsrd-cheri.github.io/cheribsd-getting-started/features/temporal.html
   * Verification-oriented model for Integer-Pointer cast VIP more powerful
     and does not mandate creation of unused invalid pointer being Undefined Behavior(UB)
   * A Provenance-aware Memory Object Model for C (PNVI-ae-udi, PNVI-ae,
     PNVI-plain, and PVI semantics for C) does not cover all the edge cases from pointers
     https://www.cl.cam.ac.uk/~pes20/cerberus/clarifying-provenance-v4.html#q3.-can-one-make-a-usable-pointer-via-casts-to-intptr_t-and-back
- 8. build system parallelization relies on "Jobserver Implementation" protocol
   * https://gittup.org/tup/build_system_rules_and_algorithms.pdf "Build System Rules and Algorithms" by Mike Shal
   * https://make.mad-scientist.net/papers/jobserver-implementation/
   * token write to child and read after finishing, if incoming data exists
   * token may get lost (S->P->C with C dying and P not providing info to S)
     and algorithm has no time bound to detect this
   * supervision (library) necessary
   * problem/design space (limitation to globally coordinate (using a local TCP/UNIX socket or something))
     https://stackoverflow.com/questions/20076916/waiting-for-grandchild-after-child-process-has-died
     + Linux prctl with PR_SET_CHILD_SUBREAPER
     + pipe to original process with descendants inheriting write end of pipe:
       closed => read-end readable for EOF
     + ptrace
     + use pid 1 or implement pid 1 functionality
     + 'pid_t pid = waitpid(-1, &status, WNOHANG)' just works on non-evil children,
       but might just hang
   * can not track processes killed via SIGKILL
   * Solution space https://monorepo.tools
   * Proposed Solution to Job server implementation:
 - 9. debug system
    * rr simulates threading and multiple processes in one process to get
    accurate performance counter timings for accurate reproducibility.
    This also rules out searching for tricky race conditions between threads.
    * see ./templates/debug_ideal and prefixes with debug_
- 10. Very good high level reasoning on ISAs ("How to Design an ISA")
  https://news.ycombinator.com/item?id=39031555
    * Nobody bothers documenting ISA including time information ?except Agner Fog?
    * Outside of embedded/security domains, there won't be a close-to-optimal
      ISA for high perf (1. big core) in the near future 2. unless completely
      new market creates demand
      + Investment costs are too high
      + No sane way to lower without efficient emulation compatibility
      inheriting the very same design flaws.
      + Formal models with explicit time information help, but few economic
      incentive and a lot incentives against beyond simulations
      + smaller ones like RISC5 may be feasible, see sel4 project
    * Learning an optimal ISA from given ones: fixing 2 out of those 3
      (1. time, 2. instruction semantics, 3. source code)
      should work, because one can learn the distribution.
      + requires something like "Multiobjective optimization under uncertainty"
      with big complexity/amount of variables (numbers missing).
- 11. arcan desktop engine semantics
    * UI toolkit
    * provided by OS / DE
    * provided via IPC ABI rather than C/C++ dynamic linking API?
    * security for isolation of graphical components
    * time semantics for the render loop via Xorg compatible protocol?
    * idea formal model possible?
    * idea xorg design
    * https://news.ycombinator.com/item?id=40767108
    * https://trace.yshui.dev/2024-06-xorg-bug.html
- 12. formal model for OS security and scheduling
  * Assuming a process Pa with schedule A with deadline a and security level 2.
  * Is affecting the scheduler a security problem, because security incidents must be
    processed imminently or with what deadlines?
  * What happens, if two processes affect dynamically any security boundaries?
  * Any form of async exceptions or signal handling (ie in Linux) kills hard deadlines.
  * At least in sel4 all properties are static with statically predefined boundaries only allowed
    for scheduling and all bets are off for abstractions on hard deadline (from what the talks suggested).
    See "Verified Protection Model of the seL4 Microkernel" by Elkaduwe et al.
    * how managing the security manager should work?
  * Indications:
    + must keep CIA (confidentialy, integrity, availability)
    + security is a static system property
    + only allowed via delegating sufficient conditioned process groups
    + caller must handle (stacking) signals (if its not a simple mask) of
    child processes (or delegated higher security process), if it would be
    allowed at all up to runtime limit.
  * Ideas to remain time bounded:
    + static specified CHERI C delegation server to delegate unforgeable memory regions
      o would sidestep authorization race condition, if exactly one execution
      unit process (thread or process) can access relevant resources
      o unclear signal handling restriction, options
        - minimal number of global signals essential to ensure program consistency
        (inspired by Windows)
          * SIGINT as user input to cancel/interrupt process ASAP
          * access and memory errors to do debugging things ASAP
        - infinite signal stacking problem
          * too weak interrupt semantics do not allow handling when to block and unblock handlers
            + reasons to interrupt an interrupt? other than debugging?
              o low cost runtime logging of errors with possible execution trace + environment
            + reasons to per thread handle of interrupts vs doing everything in main/interrupt thread?
              o per thread cleanup of resources via refiring of interrupt and
              ignoring in main thread and/or blocking in mission critical never
              to be interrupted (verified) thread doing heavy work
            + reasons to ASAP handle interrupt other than debugging things?
              o system introspection at time-critical events
            + options for internal time-critical signal handling
              o 1.1 automatically block signal -> can not detect signal during signal errors
              o 1.2 dont automatically block signal -> racy
              o 1.3 signal during signal error/signal -> signals have payload vs duplicate signal number
              o 2.1 one thread handles signals -> threads + metadata forced to be
              globally handled instead of only variable for selection "next
              thread"
                * Could this be a problem in practice?
              o 2.2 all threads handle signals, but can mask it -> unclear cause
              effect relations, very likely to have signal during signal situations
        - immediate termination SIGKILL handling for process snapshots?
          * SIGKILL would be better split into Kernel API with something like
            "HARDSIGSTOP", "HARDSIGDUMP", "HARDSIGCLEAN" to dump process memory,
            if sufficient permissions exist
        - kernel backed buffers + mask with access for high perf demands?
          * unclear if per thread or not
          * without combined syscall semantics more latency?
            + signal range thread registered thread handles signals
            + current signal handling thread could set other thread to handle signals, but only
            if thread is signal handling thread
            + process properties like signaling and resource limitations
            process global, so it makes sense to have Kernel based access
            delegation system for process resources
            + counter arguments
              o no "thread-private resources"
              o more complex handling of errors like "capability in thread lost
                due to thread termination"
              o faster access to Kernel things?
              o main thread implicitly needed to keep things simple currently
              not the case?
- 13. Continuous Integration semantics (running untrusted/less trusted processes)
  * 1. process tracking
    + Brief and short: "The Unix process API is unreliable and unsafe"
      https://catern.com/process.html
    + non-cooperating
      o must remove 'setpgid', 'setsid' permissions from child processes to
      keep all child processes in same process group to be signaled ie with
      SIGKILL
      o requires Kernel support (pledge, seccomp bpf, cgroups, namespaces ..)
      o cgroups designed for exactly this use case, ie freeze process group,
      kill it (atomically) and unfreeze it
      https://security.stackexchange.com/questions/168452/how-is-sandboxing-implemented
      o best behavior with {SET,GET}_CHILD_SUBREAPER
    + cooperating
      o signals (SIGCHLD) may get merged
      o 'waitpid(-1, &status, WNOHANG)'
      o https://cboard.cprogramming.com/linux-programming/155448-sigchld-handler-not-executing-dependably.html
      o https://www.gnu.org/software/libc/manual/html_node/Merged-Signals.html
  * 2. filesystem restrictions
     o idea
- 14. general formal model for graphics unfeasible
  * timing in graphics best effort: "lest things go kaboom almost immediately"
  * display server do to render jobs what the printer spooler did to print jobs
  * Problem statement: Multiple programs try to send documents to the printer
    at the same time. How do stop them from messing up the contents of the pages or things coming
    out interleaved or in the wrong order?
  * problematic timing input for deadlines: exceptions/signal handlers,
    rescheduling of processes and security context changes
- 15. filesystem semantics
  * There appears to be no good one handling all errors correctly
    https://danluu.com/file-consistency/, https://danluu.com/filesystem-errors/
  * https://michael.orlitzky.com/articles/problems_with_posix_acls_and_common_utilities.xhtml
    https://michael.orlitzky.com/articles/fix_busted_acls_faster_with_libadacl.xhtml
    idea summary
- 16. secure speculative execution
  * "A Formal Approach to Secure Speculation" by Cheang et al.
  * idea hackernews comment
- 17. stateful machine learning
  * kv cache etc
  * idea: spikes are like synaptic connection strengths and represent output of neuron
  * spikes express a sequence over time
- 18. TLA+ as only feasibe system to formally verifiy non-deterministic
  parallelism, states and changes over time via temporal logic
  * https://www.learntla.com/
  * TODO
  * https://softwareengineering.stackexchange.com/questions/270674/which-language-has-most-advanced-support-for-proof-based-programming
- 19. Reversible computation
  * For Real Hardware
    + CPU schedulers are hardware-based and thus relative simple. You can think
    of it as an hardware implementation of instruction interpreter programmers
    try to reason about, because programmers can not affect the exact schedule
    of instructions. In contrast to that GPUs (and related designs like TPUs or
    NPUs) tend to provide more direct ways of control over schedules to
    optimize performance for the relevant use cases.
    + Typical CPUs with schedulers don't provide that
      o "too many unsupported instructions or instructions with side effects
      not modeled properly"
      o Needs "a runtime for reversing ops"
  * For Developing (hardware/software)
    + paper "Exploiting Reversible Computing for Verification" by Burgholzer and Wille
    + idea: Use libraries to design with reversible operations for eliminating bugs
    + Hardare Devlopment Techniques
      o reversible gates
      o make fns reversible
      o use and make explicit garbage outputs
      o purely reversible

Interesting ideas
* https://catern.com/ideas.html file-descriptor API for processes
* Summary: Non-portable mess.

To me unclear if solved:
* check if linker speak has a formal model
  - https://www.cl.cam.ac.uk/~pes20/projects-2024.html
    looks like projects on DWARF debug info tests and linker speak
    tests (or oracle for corret semantics).
* check, if cakeml and compcert have linker speak verified
  - no
* Model for coroutine cancellation

Austral as language with linear type system without boilerplate (might become decent DSL)
https://borretti.me/article/introducing-austral
https://austral-lang.org/spec/spec.html#linearity
- cogent uses also a linear type system https://trustworthy.systems/projects/TS/cogent.pml.
Note: There are other more domain-specific fully verified languages, ie for message passing
compiling to C code or for signal processing etc (Simulink).

Thoughts on a theoretical optimal language:
```
Consider a language being bootstrapped (in multiple steps) from assembly.
Such language needs to have a freestanding build, link and dependency resolving
tool to impregnate every system for multiplying the fragments.
1.    can not build+link on the target: broken.
2.    can not resolve to find the ingredients: broken.
3.    too slow for the job: broken.
4.    too undefined: broken.
5.    no hotswapping and REPL for debugging: broken
6.    no usable async: broken
7.    no formal correctness proof of spec and parts: unfinished

One possible strategy:
1.    Have super simple to use + expressive + portable language
2.    Fix build scripts (simple to use + expressive)
3.    Fix dependency tracking+resolving (simple to use + expressive) => come up
      with language agnostic standard
4.    Have a sufficient user base to be reason to switch away from C codegen
      for static analysis + debugging.
5.    Define proper extensions on code plus fancy static analysis tools (borrow
      checking with comment syntax and separate AST/IR, so one has the slow borrow
      checker due to bigger AST and the fast compiler).
6.    Formal analysis interface to compiler (API to extract ASTs + PIE for analysis).
7.    World domination. Build a temple or something.

State of art:
- (Partially) verifiable are static and more simple language like 'Structured Text',
  'CompCert C' up to pure functional languages (io restricted) like TODO
- Hotswapping and/or with REPL are interpreted languages like Julia, Python, R
  and all functional + logic Programming Languages
- Static languages intended for optimizing compilers like Rust, C, C++, D
- Wrapup of midori as operating system for reduction of errors (without taking
  into account performance)
  https://joeduffyblog.com/2016/02/07/the-error-model/

System implementation details towards correctness:
- 1. Correctness requires to simplify data representation and handling (both
     serialized and unserialized)
- 2. Correctness requires to limit functionality to what is necessary
- 3. Correctness requires to have a system to limit expressivity
- 4. Correctness requires an axiomatic functional model
- 5. Correctness requires a functional proof of all related parts in an abstract model
- 6. Full trust into correctness requires a fully mechanized proof of all code
     under a correct model of hardware for all known hardware assumptions
     * Accepted tradeoff: Hardware failures are not under control of compiler,
       so the user is responsible to take them into account.

Candidates for an optimal language (independent of domain) working on at least 3 points:
- Compcert C is working on frontend verification and semantics of linker speak (3+4+7 solved, 3+5 unclear)
  * requires manual modeling with costly (manual) existential variables solving
- Rust satisfies 3, 4 for safe Rust and is kinda working on 6 (no completion
  model), but it is not simple, so it can not be optimal (no realistic chance
  to prove correctness or to reduce bug count to 0)
- Zig's strategy is currently to working on 1-3 for version1_0, 4 is missing PRO and RVO)
```

The claim of memory safe software is that the checker helps you and that the
base safety abstraction is reusable enough justifying the additional
compilation time. This argument falls apart on big scale, specific performance
+ lifetime or enforcement of weak coupling, ie during runtime.

the alternative is to use sel4/micro kernel approach of capabilities, but even
those assume you can not access all memory and/or code is trusted.
https://en.wikipedia.org/wiki/Object-capability_model

idea:
does alpha, beta, eta reduction also work for type checking in semantic analysis?

compiler backends important optimizations
- inline, unroll (& vectorize), CSE, DCE, code motion, folding, peepholes
- arithmetic optimizations for all math stuff
- eliminate redundant checks (specially null checks and bounds checks)

- typical OS permissions model: eventual consistency (signaling, no scheduler
  upper bound for security context etc) or possible race conditions. security
  depends on implementation details to be probably correct and sandboxing being
  sufficient.