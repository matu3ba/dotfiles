List of dedicated formally proved languages
- Simulink TODO context
- TODO message passing languages
- TODO automata verification
  * standard formats for automata
  * import
  * codegen
    + PLC has codegen from ladder diagrams and other graphical methods

Proof effort is quadratic to code size.
TODO paper as source

Interesting formal things of programming languages:
- 1.implicit proof-carrying code
- 2.explicit proof-carrying code
- 3. verifying compilers with its cost
  * 2 general usable and fully verified optimizing compiler implementations
      + compcert
      + cakeml
- 4. lowering languages to Isabelle or other interesting proof systems as
  compiler backends (or using codegen for source code and proof like what
  cogent does).
- 5. Incremental verification via accurate tracking of code changes in editor
  * research in this area tries to recompute how git commits have changed
    code instead of eliminating the problem at the root
- 6. atomic memory reordering semantics
  * out of thin air problem in C11 as most known example, see ./templates/memory
- 7. pointer semantics is based on Separation logic (extension of Hoare Type
  Theory with associated aliases)
   * instead of "update the value", "update the value for all associated
     aliases"
   * unsettled in LLVM blocking CHERI usage (mixed tagged pointer capabilities)
- 8. build system parallelization relies on "Jobserver Implementation" protocol
   * https://make.mad-scientist.net/papers/jobserver-implementation/
   * token write to child and read after finishing, if incoming data exists
   * token may get lost (S->P->C with C dying and P not providing info to S)
     and algorithm has no time bound to detect this
   * supervision (library) necessary
   * problem/design space (limitation to globally coordinate (using a local TCP/UNIX socket or something))
     https://stackoverflow.com/questions/20076916/waiting-for-grandchild-after-child-process-has-died
     + Linux prctl with PR_SET_CHILD_SUBREAPER
     + pipe to original process with descendants inheriting write end of pipe:
       closed => read-end readable for EOF
     + ptrace
     + use pid 1 or implement pid 1 functionality

To me unclear if solved:
- check, if cakeml and compcert have linker speak verified
- check if linker speak has a formal model
- Model for coroutine cancellation

Austral as language with linear type system without boilerplate (might become decent DSL)
https://borretti.me/article/introducing-austral
https://austral-lang.org/spec/spec.html#linearity
- cogent uses also a linear type system https://trustworthy.systems/projects/TS/cogent.pml.
Note: There are other more domain-specific fully verified languages, ie for message passing
compiling to C code or for signal processing etc (Simulink).

Thoughts on a theoretical optimal language:
```
Consider a language being bootstrapped (in multiple steps) from assembly.
Such language needs to have a freestanding build, link and dependency resolving
tool to impregnate every system for multiplying the fragments.
1.    can not build+link on the target: broken.
2.    can not resolve to find the ingredients: broken.
3.    too slow for the job: broken.
4.    too undefined: broken.
5.    no hotswapping and REPL for debugging: broken
6.    no usable async: broken
7.    no formal correctness proof of spec and parts: unfinished

One possible strategy:
1.    Have super simple to use + expressive + portable language
2.    Fix build scripts (simple to use + expressive)
3.    Fix dependency tracking+resolving (simple to use + expressive) => come up
      with language agnostic standard
4.    Have a sufficient user base to be reason to switch away from C codegen
      for static analysis + debugging.
5.    Define proper extensions on code plus fancy static analysis tools (borrow
      checking with comment syntax and separate AST/IR, so one has the slow borrow
      checker due to bigger AST and the fast compiler).
6.    Formal analysis interface to compiler (API to extract ASTs + PIE for analysis).
7.    World domination. Build a temple or something.

State of art:
- (Partially) verifiable are static and more simple language like 'Structured Text',
  'CompCert C' up to pure functional languages (io restricted) like TODO
- Hotswapping and/or with REPL are interpreted languages like Julia, Python, R
  and all functional + logic Programming Languages
- Static languages intended for optimizing compilers like Rust, C, C++, D

System implementation details towards correctness:
- 1. Correctness requires to simplify data representation and handling (both
     serialized and unserialized)
- 2. Correctness requires to limit functionality to what is necessary
- 3. Correctness requires to have a system to limit expressivity
- 4. Correctness requires an axiomatic functional model
- 5. Correctness requires a functional proof of all related parts in an abstract model
- 6. Full trust into correctness requires a fully mechanized proof of all code
     under a correct model of hardware for all known hardware assumptions
     * Accepted tradeoff: Hardware failures are not under control of compiler,
       so the user is responsible to take them into account.

Candidates for an optimal language (independen of domain) working on at least 3 points:
- Compcert C is working on frontend verification and semantics of linker speak (3+4+7 solved, 3+5 unclear)
- Rust satisfies 3, 4 for safe Rust and is kinda working on 6 (no completion
  model), but it is not simple, so it can not be optimal (no realistic chance
  to prove correctness or to reduce bug count to 0)
- Zig's strategy is currently to working on 1-3 for version1_0, 4 is missing PRO and RVO)
```

The claim of memory safe software is that the checker helps you and that the
base safety abstraction is reusable enough justifying the additional
compilation time. This argument falls apart on big scale, specific performance
+ lifetime or enforcement of weak coupling, ie during runtime.

the alternative is to use sel4/micro kernel approach of capabilities, but even
those assume you can not access all memory and/or code is trusted.
https://en.wikipedia.org/wiki/Object-capability_model

idea:
does alpha, beta, eta reduction also work for type checking in semantic analysis?

compiler backends important optimizations
- inline, unroll (& vectorize), CSE, DCE, code motion, folding, peepholes
- arithmetic optimizations for all math stuff
- eliminate redundant checks (specially null checks and bounds checks)