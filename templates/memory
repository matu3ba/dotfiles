Memory, from basic to complex.

https://stackoverflow.com/questions/4087280/approximate-cost-to-access-various-caches-and-main-memory
Core i7 Xeon 5500 Series Data Source Latency (approximate)               [Pg. 22]
local  L1 CACHE hit,                              ~4 cycles (   2.1 -  1.2 ns )
local  L2 CACHE hit,                             ~10 cycles (   5.3 -  3.0 ns )
local  L3 CACHE hit, line unshared               ~40 cycles (  21.4 - 12.0 ns )
local  L3 CACHE hit, shared line in another core ~65 cycles (  34.8 - 19.5 ns )
local  L3 CACHE hit, modified in another core    ~75 cycles (  40.2 - 22.5 ns )
remote L3 CACHE (Ref: Fig.1 [Pg. 5])        ~100-300 cycles ( 160.7 - 30.0 ns )
local  DRAM                                                   ~60 ns
remote DRAM                                                  ~100 ns
In-Detail, no graphic:
https://stackoverflow.com/a/4087331
https://stackoverflow.com/a/33065382
Very good quick overview:
http://ithare.com/infographics-operation-costs-in-cpu-clock-cycles/

0-write optimizations analysis (not used due to introducing CVEs)
- which cache the filled regions fit in
- L3 performance gets weird
- Intel l2_lines_out.silent, l2_lines_out.non_silent shows L2 cache fills and
  evicted line triggered by L2 cache fill (MESI cache states)
- silent evictions => unmodified lines in E or S state, non-silent evictions in
  M, E or S state line with E and S being made in some unknown matter
- 0-writes to cache line already 0 get partial elided
- performance goes up from 60% (Skylake) to 96% (Icelake) for l2->l3 writebacks
  and from 20% (Icelake) up to 45% L3 and 25% RAM speedups (Icelake)
https://travisdowns.github.io/blog/2020/05/13/intel-zero-opt.html
https://travisdowns.github.io/blog/2020/05/18/icelake-zero-opt.html

Data and addresses must be aligned to be loaded and stored on cpus.
See https://en.wikipedia.org/wiki/Data_structure_alignment
- CPUs work on multiple of word sizes
- CPUs have more die space for other operations and higher performance, if they
  dont need to handle non-aligned data
- Therefore, unaligned data must be shifted internally, padded, zerod, etc

Virtual memory can exist in form of "Paged Virtual memory" and "Segmented virtual memory".
The Kernel (CPU in privileged mode) and the CPU itself can access the MMU
(Memory Manage Unit), which is a hardware unit (typically of the CPU) which
does the virtual memory<->real memory handling including lookup+caching.
The MMU also prevents non-allowed access to memory regions via what is called
MPU (Memory Protection Unit) and has the TLB (Translation Lookaside Buffer) to
speed up the translation process.
"Segmented virtual memory" leads to memory management details leaking to the user
and these details make handling more involved and thus the routine slower.
Thus 'segmented virtual memory' is typically more advantages in embedded devices,
since 1. performance is not most important, 2. it allows more dense packing of
needed memory 3. stack size can be adjusted on demand and 4. each function
prologue can check the remaining free space if it would overflow the stack
(since the details are not hidden).
Small memory pages are typically 4KB, so the execution context would need to be
highly concurrent or complex to justify usage.

From https://en.wikipedia.org/wiki/Data_structure_alignment
- Alignment to (multiple) cache lines improves performance by preventing false sharing
  (unintended sharing between threads leading to cache misses or cache bouncing)
- Address translation (PCI remapping, MMU operations) works via page load + offset
- Therefore, aligned addresses (base-address + offset) lets the hardware map a
  virtual address to a physical one by substituting the higher bits instead of
  doing more costly artithmetic

Allocation classes
https://pzurita.wordpress.com/2015/06/29/memory-stomp-allocator-for-unreal-engine-4/
* memory allocation info to debug Memory 1. overrun, 2. underrun, 3. use after free
* works via tagging memory pages via OS (execute, read, write, and no-access) and
  using 2 pages with typically layout 1 for overruns or layout 2 for underruns:
  + 1. [empty space, alloc info, sentinel, allocation, page tagged as no-access].
  + 2. [page tagged as no-access, alloc info, sentinel, allocation, empty space (set with pattern)].
* Causes can also be UB https://www.gangofcoders.net/solution/what-is-a-memory-stomp/

Pointer semantics+preovenance, 3 cases:
1. known and reused (no ambiguity on usage)
2. can be derived ie from pointer casts (pick one and check, if it works)
* pointer p is end address of storage instance A and start address of storage instance B
* both storage instances A, B are exposed via ptr->int conversion with 2 ptrs a == b,
  a has provenance A, b has provenance B
3. unknown (external code and no encoding of IR etc)
potentially ambiguous cases
1. ptr->int->ptr cast + usage
2. int->ptr->int cast + usage
3. ptr->int union cast usage
4. int->ptr union cast usage
see https://github.com/ziglang/zig/issues/6396#issuecomment-1097287569

"Reconciling high-level optimizations and low-level code in LLVM"
"Exploring C Semantics and Pointer Provenance"
"VIP: verifying real-world C idioms with integer-pointer casts"

"PNVI-ae-udi tracks (1) ambiguities in provenance and (2) provenance exposure
whereas VIP does not. On the other hand, (3) VIP tracks provenance in integers
(in a limited way, for round-trip casts) whereas PNVI-ae-udi does not, and (4)
VIP relies on the copy_alloc_id primitive, which is not available in
PNVI-ae-udi.". https://iris-project.org/pdfs/2022-popl-vip.pdf

Afaiu, temporal (undereferenced) out of bounds pointers are also not forced to
be UB in contrast to C. From a brief glimpse at things it looks like the one
past the end thing originates from casted integers not remaining to have their
provenance. This model looks much simpler, but I have no clue what LLVM is or
will be doing.

3. simpler things?

CppMem: Interactive C/C++ memory model
http://svr-pes20-cppmem.cl.cam.ac.uk/cppmem/

memory models
1. synchronisation of memory from parallel execution works via fences https://stackoverflow.com/a/61711095
// Software Memory Models in C++
// memory_order | fences
// relaxed      | None
// consume      | LoadLoad, LoadStore (fences only for vars that are data-dependent on that atomic)
// acquire      | LoadLoad, LoadStore
// release      | LoadStore, StoreStore
// acq_rel      | LoadLoad, LoadStore, StoreStore
// seq_cst      | All (default order)
available via atomic_var.store(true, std::memory_order_release)
=> architecture-dependent and messy (stuff like spurious wakeups exist and costs can vary significantly)

SHENNANIGAN Out of thin air problem of C11/C++11 memory semantics
```Zig
const std = @import("std");
var X: i32 = 0;
var Y: i32 = 0;

fn inc1() void {
    const new = @atomicLoad(i32, &X, .Unordered);
    @atomicStore(i32, &Y, new, .Unordered);
}

fn inc2() void {
    const new = @atomicLoad(i32, &Y, .Unordered);
    @atomicStore(i32, &X, new, .Unordered);
}

pub fn main() !void {
    const t1 = std.Thread.spawn(.{}, inc1, .{});
    const t2 = std.Thread.spawn(.{}, inc2, .{});
    t1.join()
    t2.join()
    // out of thin value problem (below assertions could be wrong):
    // Load operations of 2 threads both see results of store operations of
    // other thread. store 37 in Y, because 37 is loaded from X, which was
    // stored to X because loaded 37 from Y, which is the value that was stored
    // in Y.
    std.debug.assert(@atomicLoad(i32, &X, .Unordered) == 0);
    std.debug.assert(@atomicLoad(i32, &Y, .Unordered) == 0);
}
```
Note: No optimization compiler implements consume ordering, because "dependent"
is hard to define and track along optimizations.

Due to the lack of ordering guarantees, the load operations of these two
threads might both see the result of the store operation of the other thread,
allowing for a cycle in the order of operations: we store 37 in Y because we
loaded 37 from X, which was stored to X because we loaded 37 from Y, which is
the value we stored in Y.

```Zig
@fence(order: AtomicOrder) void
@atomicStore(comptime T: type, ptr: *T, value: T, comptime ordering: builtin.AtomicOrder) void
@atomicLoad(comptime T: type, ptr: *const T, comptime ordering: builtin.AtomicOrder) T
@atomicRmw(comptime T: type, ptr: *T, comptime op: builtin.AtomicRmwOp, operand: T, comptime ordering: builtin.AtomicOrder) T
@cmpxchgWeak(comptime T: type, ptr: *T, expected_value: T, new_value: T, success_order: AtomicOrder, fail_order: AtomicOrder) ?T
@cmpxchgStrong(comptime T: type, ptr: *T, expected_value: T, new_value: T, success_order: AtomicOrder, fail_order: AtomicOrder) ?T
```
Rust has additionally compiler fence: 'std::sync::atomic::compiler_fence', which
effects are restricted to just the compiler.
This means that it does not prevent the processor from reordering instructions.
Potential use cases:
- 1. Unix signal handler
- 2. Interrupt on embedded systems
Because it happens on the same core, the usualy ways in which processor might
affect memory ordering dont apply.

See also https://reviews.llvm.org/D92842
For example `asm volatile( ::: "memory" )` can be replaced with
`void atomic_signal_fence( memory_order order );` and Zig might instead use
its own inline assembly flavor to get the same effect.

Via Kernel one can also forcefully inject a (sequentially consistent) atomic fence
into all concurrently running threads of the same process on Linux via 'membarrier'
and on Windows via 'FlushProcessWriteBuffers' to replace two matching fences
with a lightwide compiler and heavyweight process-wide barrier on the other side.

aarch64 as 'zig build-exe -OReleaseFast -target aarch64-unknown-linux-musl'
x86_64 as 'zig build-exe -OReleaseFast -target x86_64-unknown-linux-musl'
```
godbolt zig -OReleaseFast
Zig
export fn add_ten(num: *i32) void {
    num.* += 10;
}
aarch64
add_ten:
    ldr w8, [x0]      ; load x0 into w8 register
    add w8, w8, #10   ; add 10 to register w8 and stores result into w8
    str w8, [x0]      ; store w8 into x0 register
    ret               ; return to caller context
x86_64
add_ten:
    add dword ptr [rdi], 10   ; load from register rdi, add to it 10 and store into rdi
    ; dword ptr is called size directive dword = 32bit
    ret                       ; return to caller contex
```

```Zig
export fn store(num: *i32) void {
    @atomicStore(i32, num, 10, .Unordered)
    // identical to `x = 10;` as i32 is atomic on both architectures.
    // The same happens on other relaxed operations.
}
```
Likewise, Rmw operations such as addition can be be done often in
one instruciton on CISC, but take three instructions on RISC.
However, Rmw operations are not atomic, as they are split by the processor into
several microinstructions.

To make instructions atomic, Intel provides `lock` as instruction prefix to make
the operation atomic (add, usb, and, not, or, xor, [bts, btr, btc]).
This causes the processor to temporarily block other cores from memory for the
duration of the instruction.

Other than xadd and xchg none of the lock-prefixable instructions have a variant
to reuse the provided register.
Single bit instructions are bts(bit test and complement), btr(bit test and complement),
btc(bit test and complement), which also allow lock prefixes.

Modifying more than 1 bit can not be represented in a single x86-64 instruction
and likewise not fetch_max and fetch_min.

```
export fn atomicrmw(x: *i32) i32 {
    return @atomicRmw(i32, x, .Add, 10, .SeqCst);
}
x86-64
atomicrmw:
    mov          eax, 10
    lock xadd    dword ptr [rdi], eax
    ret
aarch64
atomicrmw:
.LBB0_1:
    ldaxr   w8, [x0]
    add     w9, w8, #10
    stlxr   w10, w9, [x0]
    cbnz    w10, .LBB0_1
    mov     w0, w8        ; .SeqCst adds this one
    ret
```

The closest thing to a compare and exchange loop on RISC is load-linked/store-conditional
(LL/SC) loop.
- store is conditional, such that it refuses to store to memory if any other
  thread has overwritten the value since it was loaded
- can retry on failure
- offers to be less precise when tracking memory changes trading more cycles
  for less memory usage and instead observe access

Later versions of aarch64 (ARMv8.1) include new CISC instructions for atomic
operations, for example 'ldadd' (equivalent to fetch_add) without LL/SC loop.
If using 'cas' instruction, then there is no differences between '@cmpxchgWeak'
and '@cmpxchgStrong' (like on x86_64).

See also "The AArch64 processor (aka arm64)" part 13: Atomic access for a more
in-depth understanding of AArch64.
```
export fn cas_weak(x: *i32) void {
    _ = @cmpxchgWeak(i32, x, 0, 100, .Monotonic, .Monotonic);
}
x86-64
cas_weak:
    mov    ecx, 100                     ; move 100 into register ecx
    xor    eax, eax                     ; set eax to 0
    lock   cmpxchg dword ptr [rdi], ecx ; compare and exchange with dword from ecx
    ret
aarch64
cas_weak:
    ldxr    w8, [x0]     ; load exclusive register x0 into w8
    cbz     w8, .LBB0_2  ; compare and branch to label at PC-relative offset
                         ; with hint not subroutine call or return
    clrex                ; clear exclusive (local record of executing processor
                         ; that addres shas had request for exclusive access)
    ret
.LBB0_2:
    mov     w8, #100     ; move constant 100 to register w8
    stxr    w9, w8, [x0] ; store exclusive register, return status into register x0
    ret
```

Cache comes from french word caché, meaning hidden. Cache coherence ensures that
the view between processors on the memory is coherence, meaning well-defined/
in a consistent state.
Protocols vary per architecture, processor model and cache level.

TODO explain MESI, MOESI, MESIF in x86_64 and/or from hardware perspective
based on "Efficient Virtual Cache Coherency for Multi-core Systems and Accelerators".

ARM is not cache coherent for flexibility, which means that each system can
choose its own coherency.
On ARM one can write instructions into memory, but due to d-cache and i-cache
not being coherent the written instructions might be masked by contents of i-cache
causing execution of old or invalid instructions.
See https://community.arm.com/arm-community-blogs/b/architectures-and-processors-blog/posts/caches-and-self-modifying-code
https://events.static.linuxfound.org/sites/events/files/slides/slides_10.pdf
https://stackoverflow.com/questions/39295261/how-to-synchronize-on-arm-when-one-thread-is-writing-code-which-the-other-thread

Timing Channels:
from "Systematic Prevention of On-Core Timing Channels by Full Temporal Partitioning"
1. In-order execution results
__Problem__ Has time channels from hardware events (RISC-V) => clearing valid cache bits not enough.
  Why Cache has secondary state (the state machine controlling cache-line replacement),
  wich can be exploited as a covert channel.
__Solution__ Temporal partitioning via reliable user space flush executing fence
  instruction fence.t, which clearing such state + guaranteeing history-independent
  context-switch latency
__Cost__ cold, unmitigated case: fence.t adds < 21 000 cycles to context-switch routine.
  => 1 GHz + a context-switch frequency of 100 Hz => over-head of about 0.2 %
  => indirect costs for applications not evaluated (used L3 cache very small)
  => fetch needed memory with ~90ns access cost from RAM
2.  Ouf-of-order execution untested
Idea (full flush may add unreasonable latency):
- 1. Spatial partitioning of "cores L3 memory area" with access permission bits for other cores
  * "L3 cache paging system" sounds slow
  * side stepping L3 cache would leak cache internals
  * think about it abit more
- 2. Temporal partitioning

https://wiki.debian.org/Hugepages
* Linux: Huge pages
  + see templates/linux
* BSD: Super Pages
* Windows: Large Pages
  + see templates/win

Pointer tagging with TBI for AArch64 https://www.linaro.org/blog/top-byte-ignore-for-fun-and-memory-savings/

Take the number 0x0123456789ABCDEF, in little endian.

If represented with the lowest memory address on the *left*:
(start ----------> end)
EF CD AB 89 67 45 23 01

But if you conceptualize it with address 0 on the *right*:
(end <---------- start)
01 23 45 67 89 AB CD EF

Which has the additional advantage of having the "direction"
of bit addresses match the "direction" of byte addresses.


Process stack
+-----------+ <- high address
| foo       | <-- XPS
|           |
+-----------+ <- low address

sub $8, %rsp
mov $rax, ($rsp)
+-----------+ <- high address
| foo       | <-- XPS
| %rax val  |
+-----------+ <- low address
pop rax
equivalent to
mov (%rsp), %rax
add %8, %rsp

Stack frame layout
TODO https://eli.thegreenplace.net/2011/09/06/stack-frame-layout-on-x86-64

https://stackoverflow.com/questions/261419/what-registers-to-save-in-the-arm-c-calling-convention
====32-bit ARM [AAPCS]
r0-r12: general purpose data registers
r13 (sp): stack pointer
r14 (lr): link register (subroutine's return address)
r15 (pc): program counter
apsr: application processor status register (conditional code flags, processor
      mode, status and ctrl info)
cpsr: current processor status register (same as apsr with access to more bits)
spsr: saved processor status register (info of processor state before system
      changed into this mode (ie processor status before exception etc)
* r0-r3 are the argument and scratch registers; r0-r1 are also the result registers
* r4-r8 are callee-save registers
* r9 might be a callee-save register or not (on some variants of AAPCS it is a special register)
* r10-r11 are callee-save registers
* r12-r15 are special registers
VFP register usage conventions:
* s16–s31 (d8–d15, q4–q7) must be preserved
* s0–s15 (d0–d7, q0–q3) and d16–d31 (q8–q15) do not need to be preserved
====64-bit ARM [AAPCS64]
* r0-r7 are parameter/result registers; r0-r1 are also the result registers
* r8 is indirect result register (xr)
* r9-r15 are caller saved register = temporary registers = corruptible registers
* r16-r17 are intra-procedure-call (scratch) registers [might be temporary]
* r18 is platform (specific) register [might be temporary]
* r19-r28 are callee-saved registers.
* r29
* All others (r8, r16-r18, r29, r30, SP)
  have special meaning and some might be treated as temporary registers.
====ARM instructions
- Arm® Instruction Set Version 1.0 Reference Guide
- Run-time ABI for the Arm Architecture 2023Q1
- ARM® Instruction Set Quick Reference Card

x86_64 Windows calling convention
https://learn.microsoft.com/en-us/cpp/build/x64-calling-convention?view=msvc-170
Call Variables
1. arguments 1-4 passed in fixed registers
2. For int and ptr these are rcx, rdx, r8, r9
3. For floats these are xmm0-xmm3 (older x87 registers not used to pass floats in 64 bit mode)
4. Arguments without 8, 16 32 or 64-bits in size are to be passed by reference/pointer
   * must also be on 16 byte aligned address
Return Variables
1. Int/Ptr up to 64 bit use rax, floats xmm0
2. Other values constructed in memory by address specified by caller
   * ptr to this memory constructed by caller as hidden first arg to fn
     or hidden escond arg following the this ptr
   * return value again in rax

See https://tbrindus.ca/sometimes-the-kernel-lies-about-process-memory-usage/
appendum 2 at bottom. For more Linux-specific details, take a look at
https://github.com/gsauthof/cgmemtime.
Assuming process A after fork/clone execs into B:
* Linux may optimize and show less memory in case pages are not accessed in procfs
* procfs memory is reset on exec in contrast to getrusage
* Posix getrusage resets counter during fork, which is even less accurate
  because spawned process A exec into B results in max(A,B,..) if more children exist
* Hence Posix mandates usage of small stub programs to measure memory somewhat
  reasonably accurate
* Results from wait4() not relevant to the callee, but parent process of callee
* Piping through processes including exit status etc would be necessary

Example:
A: mmap N bytes
A: fork()
B: N bytes are still memory mapped. maxrss=N
B: exec()
B: 0 (ish) bytes are memory mapped. maxrss is still N
B: fork()
C: 0 (ish) bytes are still memory mapped. maxrss=0 (ish)

====human
* recognized 5-10ms latency
* xterm within that bound, see https://beuke.org/terminal-latency/
* alacritty and kitty with higher throughput and next best average latency
* ghostty latency estimated around (derived from iphone measurements)
  20-30ms