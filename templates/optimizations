====resources
====types
====benchmarking

====resources
https://www.brendangregg.com/methodology.html
https://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html
https://www.brendangregg.com/blog/2020-07-15/systems-performance-2nd-edition.html
"Systems Performance: Enterprise and the Cloud, 2nd Edition"
nice links https://ankush.dev/
unclear: book on hardware perf tools

Misc noteworthy global optimization ideas
* "Reducing Code Size with Function Merging" by Rodrigo Caeteano de Oliveira Rocha
  * 11-15% compile time overhead? need to check again if correct
  * messes up debug locations
  * no fixup algorithm for debug locations mentioned

https://github.com/jnordwick/tempus
benchmarking functions
clock_gettime should already not perform a syscall for most systems + tempus
doesnt seem to check for invariant-tsc before assuming rdtsc(p) validity (or
uses lfence for non p variant)

====types
1. CPU bound
problem: the datastructure and algorithm is sub-optimal or the problem has
inherent complexity cost
* solution 1: Data Oriented Design methods
* solution 2: caching/use more memory
* solution 3: use faster method for less optimal solution
2. IO bound: RAM, disk or network leading to yielding CPU
* problem: frequent context switches on linux 1-20 microsecs
* solution 1: event-driven architecture (async-await, ui_uring, select)
* solution 2: user-space schedulers (goroutines, grenlets)
* solution 3: TODO improve phrasing, use better RAM tradeoffs
3. Memory bound
* solution 1: Data Oriented Design methods
* solution 2: less caching/use less memory
* solution 3: use other method for less optimal solution

====Simple Measuring how IO bound
time app.exe args
user_time user sys_time system percentage total_time total
12.67s user 1.45s system 91% 15.482 total
with total = user_time + sys_time + wait_time,
perc_wait_time = 1 - (user_time + sys_time)/total = 1 - (12.67 + 1.45)/15.482 = 0.088
wait_time includes: disk+networking IO waiting, explicit sleep calls, lock waiting,
scheduling delays from kernel
=> workload defines if process is IO bound

====Advanced Measuring how IO bound
https://github.com/iovisor/bcc
 -K (kernel stack), -p pid, 5 (seconds)
offcputime -K -p `pgrep app.exe` 5

====benchmarking
https://ankush.dev/p/reliable-benchmarking
* Simultaneous Multi-Threading (SMT, CPU simultaneously executing 2 or more threads using 1 core)
* Boosted clock frequencies (Temporary boost on few cores aka "Turbo Boost")
* Thermal throttling
* Scheduling noise, other running processes
https://github.com/ashvardanian/less_slow.cpp

Disable SMT
sudo cat /sys/devices/system/cpu/cpu*/topology/thread_siblings_list
disabled_cpus=(1 3 5 7 9 11 13 15)
for cpu_no in $disabled_cpus
do
  echo 0 | sudo tee /sys/devices/system/cpu/cpu$cpu_no/online
done

Disable dynamic clock boost
enabled_cpus=(0 2 4 6 8 10 12 14)
for cpu_no in $enabled_cpus
do
  echo 2700000 | sudo tee /sys/devices/system/cpu/cpufreq/policy$cpu_no/scaling_max_freq
done

Better scaling governor
enabled_cpus=(0 2 4 6 8 10 12 14)
for cpu_no in $enabled_cpus
do
  echo performance | sudo tee /sys/devices/system/cpu/cpu$cpu_no/cpufreq/scaling_governor
done

Remove Scheduler Noise
taskset, cset https://documentation.suse.com/sle-rt/12-SP5/html/SLE-RT-all/cha-shielding-cpuset.html
netlink/CN_PROC_IDX protocol to notify "task forks during move and its child remains in the root cpuset",
but it's difficult to handle all events correctly to make this bulletproof (libcgroup has been using it and there were several fixups).
dont rely on asynchronous migration in general and use configuration that starts processes with respective affinity.

Disable ASLR
echo 0 | sudo tee /proc/sys/kernel/randomize_va_space

Warmup and Cooldown
* keep energy connection
* between two runs, let processor temperature recover to reasonable numbers
* long runs => warming up by running benchmark without not measuring (CPU and page cache)
* short runs => drop caches before each benchmark run

Mechanical Sympathy => no death by 1_000 cuts
