====inspiration
====optimal_repl
====system_abi
====debugging_definition
====library_framework
====interested_discussion

====ideas

====inspiration
https://github.com/libdebug/libdebug
hypervisor based debugging TODO
each system component has own debugging methods, ideally to be stored and used from own or other functional device
* 0 broken partition table or disk offset, encryption table
* 1 efi stuff (if applicable)
* 2 grub/bootloader
* 3 user login database
* 4 disk too full -> not being able to switch generations or use any regular binary
* 5 broken system configuration
* nixos/guix etc: N system configuration snapshots (not kernel+gpu driver!)
  - guix still slow, incoherent, less support/maintenance than nix for hardware
* nix store artifacts binary cache != manually built ones

====optimal_repl
* hot reload https://zig.news/perky/hot-reloading-with-raylib-4bf9

====system_abi
* STATUS Huge pile of calling conventions=protocols in systems in the field
  - almost all of them never designed with abi versioning in mind
  - elimination requires building competing platforms/protocols to replace
    existing ones while being compatible for the meantime
* MISSING_PART Platforms with calling conventions offer no libraries to introspect themself
  for user code debugging at 1. compilation time, runtime, linktime etc
* RESULT Each compiler, debugger etc implements the same stuff again and again
  without a reference implementation
* HISTORY C ABI had C compiler as reference implementation
  - does not help with introspection
  - C ABI design was no priority at first
  - crufty tool to bootstrap was never improved
* THE_FIX Bootstrap improved tool back to lower level/base layer
  - make it trivial to bootstrap from and for any platform

====debugging_definition
From https://gu.outerproduct.net/debug.html
Assume: developer/user knows correct and incorrect states and the code
represents a correct model of the intended semantics.
* program as state machine (often non-deterministic)
* bug is bad transition rule
* execution witness as states and state transitions encountered on one specific
  program run
* if execution witness shows "bad state", then there must be a bug,
  - often not obvious when the state became bad like when state is subtly wrong
* debugger as query engine over states and transitions of a buggy execution witness
* most programs behave non-deterministic
* frequent operation is bug source isolation to deterministic component
  - encapsulation of non-determinism simplifies process like functional programming (isolation of all side effects)
* concurrent code tricky to debug, because one needs to trace the execution flow
  to estimate where the origin to the incorrect state is
  - time-travel debugging/deterministic replay with rr only allows stepping backwards,
    but simulates multiple processes or threads on 1 CPU core
* suspecting scalable deterministic replay with relative low time overhead
  on regular hardware via thread-locality (based on 2002 paper) without further
  examples

====library_framework
What's your imagination to what time-travel debugging can be? (Egg-laying wull
milk pig with mind-blowing complexity and scope).
* graphical + text editor with shortcuts to serialize and deserialize
  - 1. program control flow state by relative conditions to another (in what
    source file area which thread or process is)
  - 2. program state state conditions (relative or absolute program conditions)
    and ideally would generalize both to
  - 3. arbitrary programs [in which for example kernel information like memory
    usage could be queried] for all introspection use cases (debugging,
    memcheck/validators, race condition checkers, tracers) to have seperate
    overlays for debugging to reproduce the state of one program state
    condition within the other tool even under stochastic events.
* As far as I understand, currently language semantics leak into debuggers
  besides debuggers/systems with repls can not be designed well for third party
  interactivity, which is why gdb and lldb do not expose pretty printer as c
  api or formatting library etc.
* More ideally, the semantics of the program itself could be reasoned
  about, but this would require compiler support with complete enough memory
  models + simulator with debug api etc on top of that (say Miri or Cerberus)
  to initialize the simulator from arbitrary program state.
* As of now, we neither have a cross-usable snapshotformat for programs (on
  the same platform) nor tooling to compare program states or traces.
* Further, debug system scripting experience is very bad
  + non-portable
  + natvis (must create bindings), python/javascript (no underlying type info)
* Kernel mode debug traces are not supported (unfeasible to support in general),
  think of combined user land kernel land trace across processes

====interested_discussion
* Lauterbach or ericsson context may be most advanced regarding debugging techniques
* ui missing and classes of problems to debug
  - problem source classification for debugging
  - examples: missing session stuff, cerberus/miri/more concrete tooling and user annotation (program invariants, filters and stuff like that)
  - comparing continuations mandates something like miri/cerberus or an equivalent with more optimizations for threading problems.
  - not sure if bmc of cerberus does that.
  - only having queries is not powerful enough to trace down things. in the end you have to write something like implicit CTL formulae
  to reduce debugger interactions.
    + capturing or checking state may introduce or prevent detecting errorneous behavior
* recording traces of multiple runs of same program into queryable database could already open new interesting ways of debugging
  - see above portable format for comparing tools and reexecute at individual timepoints (known to reproduce a buggy state)
  - so far only done via coverage analysis and other statistical tools
  - combine such trace db with fuzzer as state space crawler and you get powerful insights on execution
  - digging down one trace hard to extrapolate and imagine causality for certain states
  - uncertain how to combine information from traces, meaning what queries one can make against this info
  - dynamic trace db would be useful for fuzzer itself, to find unexplored states
    + probably means that this is already being done by antithesis for example
  - equivalent state collapse
    + see above user annotations as runtime checks for beahvior + efficient subset of CTL+ formulae
    + dependency graph construction, view, annotations over states etc to solve questions like
      "what state lead to this state"? "what states this state can evolve to?" "what transitions could lead to this state?"
* 'semantic dependency'/sdep relation
* dynamic hash-cons. union-find -> eventually coherent
  - tricky when you have state which is somehow parametric but uniform in structure—have
    to pick the right representation to be able to compress it properly
* better framework, better defaults
  - debugger for avoiding harness and a better starting state
  - reverse execution for reducing startup costs
  - coverage guided fuzzing as default
  - symbolic execution to generate better inputs
  - arcan appl for simluating interaction
  - basic triage then into a12+cat9 for interacting with unique triaged source
  - senseye for comparing more subtle data corruption (packets in packets, string injections, ...)
* debugging as process to learn memory relations and runtime context relations (t1 does x while t2 does y)
  - one of the simplest initial steps is to add invariants to the program for synchronization stuff.
  - very often removing program parts is no option due to weird dependencies and effects on timing behavior etc.
  - Sampling another method, if the problem is less time and more arithmetic/offset related or stochastic.
https://github.com/rr-debugger/rr/blob/ca36c98ab63f54f777cac4f962c5c9e059022d41/src/fast_forward.cc
https://github.com/rr-debugger/rr/blob/ca36c98ab63f54f777cac4f962c5c9e059022d41/src/ExtraRegisters.cc
* store trace information for any tracing levels
  - instruction tracing
  - function tracing
  - library/service interface level tracing
  - service to service tracing,
  - machine to machine tracing
  - https://www.philipzucker.com/hashing-modulo/ egraphs-style heavy hammer 'represent the big thing in compressed form'
  - another option is sometimes available 'compress by construction' which is nicer when it works
* practicality/scalability (aka trace is too big/slow, does not reproduce problem etc)
  - (semi-) deterministic record+replay, because non-determinism breaks any hashtree for looking up traces
  - jitted/compiled tracing/sampling or "make things semi-deterministic" depending on use cases
  - 1. exploration requires storing as is (default/selection of trace profile what to record)
    vs 2. collecting transitions that lead to the exact same state (chopping of states)
    vs 3. higher level reasononing or larger collection of accumulated traces implies
    normalisation or other techniques mentioned in the hasing modulo theories article
* what should state be and how to model compound states like structs and arrays?
* how should non-determinism detection work?
* Debugging, like programming, has fundamental tradeoffs between determinism, performance, memory and synchronization speed/observability.
* stochastic events like race conditions depending on memory access + speed.
* rewrite binaries to trap syscalls via qemu libc + stack swapping
  https://lobste.rs/s/wujuzj/think_you_can_t_interpose_static_binaries
  - x86-64 with 2 bytes syscall instruction, using
    + lazypoline based on paper "System Call Interposition Without Compromise"
    + zpoline based on paper "zpoline: a system call hook mechanism based on binary rewriting"
    because
    + reliable disassembling binaries being hard "An In-Depth Analysis of Disassembly on Full-Scale x86/x64 Binaries"
    + ptrace requires jumping between kernel and user space up to 5 times
* system-theoretic process analysis (stpa), CAST (Causal Analysis based on System Theory)
  https://psas.scripts.mit.edu/home/books-and-handbooks/
  https://entropicthoughts.com/aws-dynamodb-outage-stpa
  - STPA
    + 1 Define purpose of analysis.
    + 2 Model the control structure.
    + 3 Identify unsafe control actions.
    + 4 Develop loss scenarios.
  - CAST
    + TODO
* semi-crazy opinion(s)
  - windbg has no sane library api, which makes it by construction unsuitable
    for automation and inspection.
  - I'd personally prefer, if we would have the options of multiple time-travel
  debugging sessions based on synchronization points (if needed) being overlapped
  to single out bugs in interesting areas (not covered by static or dynamic
  analysis). Debugger would be essentially a virtual program slicer being able to
  hide unnecessary program parts or reduce it. However, so far we neither have
  the options for piece-wise time-accurate multi-threading recording, nor slicing
  overlays nor in-memory reducers (ideally also reducing AST). I may be mistaken
  on "piece-wise time-accurate multi-threading", since I've not checked what is
  possible with hardware yet.
  - Heck, we dont even have overview data for common bug classes and cost for
    verification or runtime-checking in academia etc.
* Q: What kind/class of issues are caused by "thread interleaving on the scale
  of nanoseconds"? Faulty CPU bit flips due to radiation/quantum effects or what are
  you referring to? Just curious.
  A: Not doing things atomically when they should be (incl. missing locks around
  tiny ops) would be a pretty large class.
  With native multithreading data can pass from thread to thread millions of
  times per second, and you're much less likely to hit obscure interactions when
  limited instead to maybe a hundred to a thousand context switches per second.
* crazy opinion(s)/ideas I'd like to have somebody solve at some point and
  likely should not be attempted without simpelr ISAs
  - inspired by "Distributed Order Recording Techniques for Efficient Record-and-Replay of
  Multi-threaded Programs" ideas, but elimination of huge state and timing space
  to find bugs in multi-threaded/distributed programs:
  - afaiu LLVM is unable to express relative ordering relations for optimizations,
  see https://llvm.org/docs/Atomics.html
  volatile means "dont optimize this instruction/access out" in thread
  atomic means "dont reorder across to these rules" between threads
  i can not express if some section should be intentionally racy or how memory
  orderings between different code sections should work as a graph
  and instead have to reason on each small code section
  I'd like to express memory relations as graph for better debugging
  capabilities and to workaround atomic model limitations.
  - yeah, I'd like to have no shadow memory on every access and log each access
  instead for later rerunning/minimizing overhead. the algo description does not
  allow static generation of the possible runs or any sort of recording
  https://github.com/google/sanitizers/wiki/threadsanitizeralgorithm
  "Finding races and memory errors with compiler instrumentation."
  AddressSanitizer, ThreadSanitizer. by Konstantin Serebryany, Dmitry Vyukov
  "A Binary-level Thread Sanitizer or Why Sanitizing on the Binary Level is Hard"
  by Schilling et al.
  "ThreadSanitizer – data race detection in practice" by Konstantin Serebryany and
  Timur Iskhodzhanov
  - tsan terminates the program when the state machine runs into a race, so there's
  only one possible program run under it. but i guess if you were to remove that
  restriction, yes
  - it does allow recording though, shadow words describe a pseudo-timestamp, and
  you could just record those timestamps to build up a model of the accesses that
  happened. or do you mean something else when you say recording?
  - yes, I'd like to 1. record the multi-threading program itself. recording the
  memory accesses may not necessary reproduce in a long-living program with tsan.
  also, I'd like to 2. be more explicit in what stuff to sanitize, but the only
  quick example for now that comes to my mind are intentional race conditions.
  further, I'd like to be able to 3. simulate program execution and program
  fragments via 4. dumping parts only accessed by one thread or a group of
  threads. Ideally, I'd be able to 5. reset the program state and continue
  different thread schedules like fuzzing.
  - My understanding is that 5 is not feasible without programmer input of how
  things should relate to another in graph form, same for 4. 3 may be possible, 2
  might be seen as acceptable trade-of. Any opinion?
  - an interesting idea. this sounds a lot like deterministic simulation testing,
  but in this case for races. antithesis is working on a deterministic hypervisor
  for running general programs and then rolling back the state to reproduce bugs,
  but their's works in a single-threaded fashion, there's no true parallel
  execution in it, afaik. how would you "force" a race to happen in this
  scenario? in the case where you have two operations racing in a single point
  in time, how do you tell the simulation who's input to consider first. not
  only that, but another problem you'll have is defining values after the data
  races happens. by definition right now, the value after it is unspecified,
  and tsan doesn't need to deal with that problem either because it just halts.
   > how would you "force" a race to happen in this scenario?
  for the "race check": i think its ok to assume correct aliasing and bounds.
  the programmer would need to define what memory regions would be accessed by which thread group and in what relation.
  checks would then start from a a 1.program state, 2.input sequence, 3. thread choice sequence, 4. timeout and history be compared
  another approach would be to abstract away memory and only fuzz check logical access, but that would go into proof area
  - ah ok, so you're manually defining the thread choice sequence in who wins the race correct?
  I'd do it semi-automatic, but yes.
  - my main concern was that they dont provide a brief overview of perf cost and race condition detection.
  so it felt like some "do something with vm stuff", which is often too slow to detect race conditions
  be it in kernel or in user space unless you have a slower language like go
  https://antithesis.com/docs/
