see for impls and techniques ./example/parallel_programming/README.md

Based on talk by kprotty "Proficient Parallel Programming"
parallelism: doing 2 or more things at the same time
concurrency: doing 2 things not necessary at the same time
OS provides parallelism on top of concurrency.

Why?
- Getting things done faster
- Success stories
- Speed up existing slow code
Why not?
- Tasks rely on each other
- Entire system needs to change
- Given concurrency solution exists (SIMD, non-blocking IO[io_uring,..], GPU,..)
- Single threaded fast enough

Cost comparison
- sort 100 items: 3us
- thread wakeup: 5us
- thread spawn: 25us
- sorting 1000 items: 45us

Use cases
- more IO
- more computation

Dilemma
- single threaded and deterministic (best for fuzzing and simulation testing)
- trace of program run as flamegraph
  * shows major blocking parts
  * problem: sorting => solution: faster, incremental
  * problem: hashing => faster hashing

Preparation
- Measure performance with benchmarks
- profile to see whats worth offloading
- reach for existing solutions

Mindset
- What are you willing to use? SIMD, threads, entirely different language ie Futhark for GPU programming
- different rules:
  * duplicate work > communicating
  * read eagerly > write
- avoid 3 sins
  * spinning (lyign)
  * dependencies (stealing)
  * contention (useless beating)

Spinning
```md
while not check(state):
  ~~continue~~
  ~~pause unbounded~~
  ~~yield()~~ Most impls of yield only do check current code. yield to unrelated thread also bad.
  ~~sleep(N)~~ Good for throughput, bad for latency.
```
Use events instead (Signalling > Polling unless low level driver things)
# consumer                 |  # producer
while not check(state):    |  ready(state)
  event.wait()             |  event.set()

# wait                     |  # set
threads.push(self)         |  ready(state)
sleep()                    |   .pop_all()
                           |   .for_eacH(wake)
Problem Thundering Herd (All threads are woken up to hammer CPU)
Solution Change to chained wake

Competition
Run as many tasks as you can
Race to register pattern (once, CPU pipelining):
- doing work nonetheless and the first one is allowed to write
- minimize time in critical section
- avoid (deferred) handoffs
  * good for fairness
  * bad for throughput
- chennal > mutex, because its easier to mess up size of critical section
- WaitGroup instead of multiple join(): do 1 syscall instead of N syscalls
- EventListener > ConditionVariable: wait for arbitrary condition without walk through mutex
- Channels too much overhead: Prefer RwLock
  * instead of Mutex: Writers block everyone, Readers only block Writers
  * RCU instead of RwLock for specific high performance cases
    + readers dont block writers at all
    + single writer to swap out the item

Contention
- resolving collision is main performance killer of threads
- cas loop
```
old = atomic_load(ptr)
while true:
  new = old + 1
  old = atomic_cas(ptr, old, new) catch break
```
- for simple instructions use the hardware primitives
```
old = atomic_fetch_add(ptr, 1)
```
- too complex operation
  * use backoff operation for ca. 2x throughput
```
while true:
  old = atomic_load(ptr)
  new = complex(old)
  _  = aomitc_cas(ptr, old, new) catch break
  backoff()
```
- lockfree queue
  * check length before (reading can be quick)

```
# push(itm)                                   | # pop()
with lock(mutex)                              | if LOAD() == 0:
  array.push(item)                            |   return error.Empty
  STORE(len(array))                           | with lock(mutex):
                                              |   defer STORE(len(array)
                                              |   return array.pop()
```
- lockfree queue: only use STORE, if pop did return an operation to prevent write collision resolution
```
# push(itm)                                   | # pop()
with lock(mutex)                              | if LOAD() == 0:
  array.push(item)                            |   return error.Empty
  STORE(len(array))                           | with lock(mutex):
                                              |   item = try array.pop()
                                              |   STORE(len(array))
                                              |   return item
```
- Partition
  * futex: [N]Mutex<WaitQueue> as table of wait queues
  * dashmap: [N]RwLock<Hashmap> a hashmap, but a lot of them
  * align(cache_line) to prevent cache line contention
  * batch operations before contending (allocatores and injector do these)

- Optimize single threaded first
- Measure things for a strategy
- Try existing solutions
- ASIC
  * accumulate
  * satiate
  * isolate
  * consolidate

ABA_problem
- thread A does something, thread B interleaves but cleans up afterwards,
  thread B never notices that thread B has run

General
* best class of algorithms is wait free (meaning waiting an as low as possible and bounded time, usually quadratic to number of threads)
* all recent high performant algorithms use hazard pointers as vector/array of [0,MAX_THREAD_ID]
* getting the size, if the storage is non-continuous, has a perf cost
* reliably detect, if a pop emptied a data storage, on the same thread after the pop, has a perf cost
* vice versa detecting that the storage ran full
* most papers have no reference implementation with tests linked or can have
  non-permissive licenses, so searching for them can be annoying
  - reasonable code source https://github.com/pramalhe/ConcurrencyFreaks
    + no tests, but code looks good
  - papers usually use raw pointers and user is expected to adjust to used
    external or internal storage
    + no overview papers of space vs memory vs works on weak memory tradeoffs

win
  DWORD GetCurrentThreadId();
  idea ntdll call from zig
posix

State of Art
* "Wait-Free Linked-Lists" Shahar Timnat, Anastasia Braginsky, Alex Kogan, and Erez Petrank
* "wCQ: A Fast Wait-Free Queue with Bounded Memory Usage" Ruslan Nikolaev, Binoy Ravindran
Unclear status
* https://github.com/erez-strauss/lockfree_mpmc_queue
  + missing optimizations "relax the strong ordering of the atomic operations"
* CRTurn Queue - The first MPMC memory-unbounded wait-free queue with memory reclamation
  https://github.com/pramalhe/ConcurrencyFreaks
Simple
* "A Pragmatic Implementation of Non-blockign Linked Lists" Timothy L. Harris
  https://github.com/hd-zhao-uu/Non-Blocking_Linked-List
* https://github.com/rigtorp/MPMCQueue bounded multi-producer multi-consumer concurrent queue written in C++11 Erik Rigtorp
* "Simple, Fast, and Practical Non-Blocking and Blocking Concurrent Queue Algorithms" by Maged M. Michael and Michael L. Scott.
  https://github.com/Qarterd/Honeycomb/blob/master/src/common/Honey/Thread/LockFree/Queue.h
  https://www.cs.rochester.edu/research/synchronization/pseudocode/queues.html
