https://lwn.net/Articles/995276/#Comments
todo summarize
https://mutants.rs/
https://www.fuzzingbook.org

Best Practice taken from "Large Scale Architecture: The Unreasonable
Effectiveness of Simplicity - Randy Shoup" and extended by personal experience.
Related and partially very funny https://grugbrain.dev/
"Fear Of Looking Dumb (FOLD), grug also at one time FOLD, but grug learn get
over: very important senior grug say 'this too complicated and confuse to me'"

Test-Driven Development
* inbox zero for bugs (immediately fixed, when they come up)
* feature flags to [dis|en]able feature for particular set of users
=> decouple feature delivery from code delivery

"All of the pain that we feel when writing unit tests points at underlying
design problems."
=> Integration Testing is hard (especially time scaling for simulation + translation to real system).
=> Unit testing *must* be easy.

Continuous Delivery
* iteratively identify and remove bottlenecks for teams
* "what would it take to deploy application every day?"
* 3x-5x improvements in deployment frequency, lead time, change failure rate, mean-time-to-restore
* configurable email notification on failures
* automatic running on code additions (both locally and over review platform)
* configurable text file
* blocking jobs logic to prevent usage of (multi) system resource usage
* upper limit definitions
* explanation of overall system design
* at least user-based and ideally least capability-based compartmentalization for security
* being portable (at least with virtualisation ie for security) and open source for customizability and code quality
* understandable and machine readable built artifact reasoning logs
* tamper prevention and signing of artifacts
=> security setup must be user configurable aka user must be admin on bare metal machine

Simple transactions
* Don't use "distributed transactions".
* Model transaction as state machine of successive atomic events (workflow)
* Roll back with compensating operations in reverse
* Explicitly expose intermediate states in the interface

* Consider event-driven "dataflow" depending on architecture

Known bad designs:
- 1. forcing of default constructors
  * use injection instead
- 2. qtest / test frameworks are very verbose
  * use own test framework or language ones, unless specific use cases can be isolated
- 3. data driven tests via macros is the same as using a table without understandable error messages.
  * use a table + for loop instead
- 4. tight coupling of unrelated components
  * use weak coupling, if feasible
- 5. unnecessary classes and other boilerplate
  * don't decouple strongly related things, because classes create costs via pointer indirection
  * moving around code takes time
- 6. code review of most important logic, regarding overall design, good practices and testing
- 7. Unreadable error context + diagnosis (google mocks)
- 8. Unit test or component test frameworks are complex, have no single known concise documentation page for usage (google mocks)
     inclusive or have no good keywords for searching the use case

Related (https://12factor.net/) for web and service appliations (without high performance or latency constrains):
- 1. Codebase One codebase tracked in revision control, many deploys
- 2. Dependencies Explicitly declare and isolate dependencies
- 3. Config Store config in the environment
- 4. Backing services Treat backing services as attached resources
- 5. Build, release, run Strictly separate build and run stages
- 6. Processes Execute the app as one or more stateless processes
- 7. Port binding Export services via port binding
- 8. Concurrency Scale out via the process model
- 9. Disposability Maximize robustness with fast startup and graceful shutdown
- 10. Dev/prod parity Keep development, staging, and production as similar as possible
- 11. Logs Treat logs as event streams
- 12. Admin processes Run admin/management tasks as one-off processes

Unit tests
- must never depend on timing (deadlines or waiting times)
- must be as independent as possible from external factors excluding the tested ones

Component/partial integration tests
- must have clear visible distinguishment including separate build step to unit tests
- ie mocking must justify by test description 1. long compilation, 2. long testing times, 3. existence
  + example: usage of network component ie writing data over socket must be justified

Integration tests via
- Simulation on long hardware timeouts: plan timeout length and scale timeouts
  by maximum allowed factor to be able to represent minimal timeout without
  affecting system behavior (justify minimal timeout and/or why time scaling not possible otherwise)
- Simulation: remove network unless testing network behavior intended and justify why

Design
- specifications (Lastenheft) with basic supported use cases
- requirements (Pflichtenheft) for timing behavior (initialization, deinitialization, maximum latencies, waiting times)
- clean termination for good automatic error diagnosis in most cases
- system resource supervision after basic initial implementation (timing behavior, memory etc)
- justification of permanent latency introductions (threads) (in real time system all temporary latency must be justified)
- justification under what constrains and why system remains consistent (see definition for data base)

Build system
- correct
- as fast as possible
- caching
- cache clearing
- easy cross compilation
- easy cross platform simulation
- easy packaging and package usage

Meetings
- all important decisions should be mathematical and understandable
- pre and postreview of relevant things via overall transcription
- time slots for speaking
- slides with short talk, if understanding is important
- in doubt, unless it is about business/money idea, YAGNI (You aren't gonna need it.)
- separate informative from non-informative parts

Materials
- ssh has >0.2s delay per key press, virtual machine is worse
- headset, ergonomic keyboard, set of good writing material
- if feasible: comparable (embedded) devices for playing at home
- fixed places and system for where items are to remain and database for lookup items

Ideas for the ideal cli test system for non-timing and timing based testing:

- design rules to distinguish observable behavior without timing from
  timing-related behavior and how to work around that
- design rules for making a multi-threaded/process tool (piecewise) deterministically
  to write test queries for the collected data of the behavior
- test system for multi-threaded/process time based test collection and human output

- Query system options for test behavior: You are basically writing a RAM database
  best TODO intro
- Make it stupidly simple.

TODO explain property based testing, metamorphic testing
https://github.com/cryptocode/marble
https://www.cockroachlabs.com/blog/metamorphic-testing-the-database/

"Compile-time function call interception for testing in C/C++" by Gabor Marton and Zoltan Porkolab
for some terminology
function call interception (FCI) methods:
- Load-time FCI (LD_PRELOAD unreliable with member functions)
- Run-time FCI (ptrace not portable, cannot instrument inline functions)
- Pre-compilation-time FCI (macros may be uncompilable or have hazardous side effect)
- Link-time FCI (wrap in linker to use wrapper for specified symbol unusable if
  symbol is in same translation unit where referenced)
- Post-compilation-time FCI (Bnary Rewriting and Call Interception via disassembling and adjusting program)
- Compile-time FCI (gcc/clang -finstrument-function to instrument all fn calls
  with code before and after body of fn with shortcoming that intercepted fns
  can not be replaced)
```h
    void __cyg_profile_func_enter(void *this_fn, void *call_site);
    void __cyg_profile_func_exit(void *this_fn, void *call_site);
```

four-phase test pattern:
- setup system under test (SUT)
- SUT runs
- compare expected outcome
- teardown SUT
given-when-then pattern
- given precondition
- when specified behavior
- then expected changes
=> test setup should be strictly part of the visible test code
A seam is an abstract concept introduced by Feathers to identify points where we can break dependencies
(if modifying the source code is not an option):
Preprocessor, Compiler, Object, Linker

'Compile-time FCI seam' looks like googlemock with EXPECT_CALL, MOCKMETHOX.
'Post-compile-time FCI seam'
"FCI with Call Expression Instrumentation" is not contextualized, but looks
like 'Run-time FCI seam' due to runtime setup of replacement function, and
'Compile-time FCI seam' due to using compiler intrinsics. 1 compiler
instrumentation module and 2 runtime library. 1 modifies code to check whether
function has to be replaced, 2 provides fns to setup replacements.
SUBSTITUTE(D::foo, D_fake_foo); is implemented via __function_id, but only in
gcc.
'Post-compile-time FCI seam' is not contextualized.

Injection combines the complexity/flexibility of callbacks with
configurable/additional state.

====mocking
More of an WIP idea after discussion at work, for which this issue is the closest thing I could find.
Identified use cases of mocks in C and C++:
  workarounds due to no error definitions to have at least some test coverage of error conditions
  count function calls (and sequences of executions) as poor-man minified execution trace to not being forced to enumerate the possible/necessary states for a partial runtime trace (which can be painful for some standards/business logic)
The first is not needed, because we have explicit error semantics.
The second requires either to either 1. have some function/runtime tracer lib for counting logic on traces or 2. comptime extension of containers + comptime wrappers for function calls and (if necessary) linker scripts to substitute the functions.
I know, very hacky. However, it should be (very) useful to test network stuff or anything to eliminate/abstract slow Kernel calls.
Alternatives ?
Related use case: Helpers to create test case from object state (ie from gdb) and a fast way to make zig code from it to lazy-create a test case.
Generating with from gdb snippets and use lsp to generate an objects instance and fill it with values would be leet.


====automation
vendor specific frameworks, since solutions are for scaling configurations, scheduling of connecting things
twincat testing - https://tcunit.org/#/
https://alltwincat.com/2020/06/15/the-five-best-and-worst-things-with-twincat/

https://antithesis.com/docs/best_practices/sizing.html
TODO best practice

https://lcamtuf.blogspot.com/2014/10/fuzzing-binaries-without-execve.html
fuzzing ideas + process semantics
https://skia.org/docs/dev/testing/automated_testing/
https://github.com/luci/luci-py/blob/main/appengine/swarming/doc/Design.md
https://github.com/luci/luci-py/blob/main/appengine/swarming/doc/Detailed-Design.md
