====llms
====artifacts
====known_to_work_use_cases
====visualization
====frameworks

====llms
* fast selection of models via https://github.com/jedisct1/inferswitch
* reasoning about llms https://transformer-circuits.pub/2025/attribution-graphs/methods.html
* https://newsletter.semianalysis.com/p/tpuv7-google-takes-a-swing-at-the
* https://newsletter.semianalysis.com/p/google-ai-infrastructure-supremacy
* https://globaltechresearch.substack.com/p/the-ironwood-an-introduction-to-google
* https://techaimint.substack.com/p/tech-talk-how-googles-tpugemini-push

====artifacts
https://ea.rna.nl/2024/05/27/when-chatgpt-summarises-it-actually-does-nothing-of-the-kind/
* sumarizing
  - parameters from training material
    o subject is well-represented by the parameters => parameters dominate the
    summary more and the actual text you want to summarise influences the summary
    less
    o bad in making specific summaries of a subject that is widespread
  - context (prompts and answers until now in chat) including the text given to summarise
    o context very small => little influence, so result dominated by parameters
    o context large enough and subject not well-represented by params =>
    text to summarize dominates results resulting in "text shortening", not
    true summarizing
  - no intelligence involved, query machine with context for processing
    o Intelligence can be thought as the capability to process and formalize a
    deterministic action from given inputs as transferable entity/medium.
    In other words knowing how to manipulate the world directly and indirectly via
    deterministic actions and known inputs and teach others via various mediums. As
    example, you can be very intelligent at software programming, but socially very
    dumb (for example unable to socially influence others).
    As example, if you do not understand another person (in language) and neither
    understand the person's work or it's influence, then you would have no
    assumption on the person's intelligence outside of your context what you assume
    how smart humans are.
    ML/AI for text inputs is stochastic at best for context windows with language
    or plain wrong, so it does not satisfy the definition. Well (formally)
    specified with smaller scope tend to work well from what I've seen so far.
    Known to me working ML/AI problems are calibration/optimization problems.
    o I think intelligence is having at least some/any voluntary thinking.

====known_to_work_use_cases
given known probability distribution(s) and sufficient handling of edge cases (controlled environment or enumeratable enough)
* 1. copy-paste/use common optimal learned goal
  - claude for more brute-force like editing and local simplifications
  - coderabbitai for context-aware/learned analysis to review code
* 2. automate calibration-like tasks
  - diy llms (+ textual frontend, progress etc)
* 3. basic overview stuff not to be trusted/needs validation
  - deepseek, chatgpt, gemini
* 4. derived formal system by given system traces via SMT or concolic testing
=> pattern detection (like word contextualization) + optimization problems (like calibration)

any automation system can only be as good as the (formal) semantics encoding
it, so dont expect much without (formal) encoding of meanings or copy-paste
solution.

====visualization
https://bbycroft.net/llm

====security
LLM context security very problematic
- context files given to LLMs are not cryptographically signed leading to easy exploitations
- state filtering and extraction probabilistic at best, not deterministic

====frameworks
https://github.com/Open-Model-Initiative/Omni-RLM
* very early
* no ideas on security model yet
* plan to build RLM planner
* planner is in the sense of agent task planning, not really k8s style of
  things, but it does depend upon how you deploy the agent/llm cluster
* Kubernetes execution model with the very involved configuration complexity?
* preventing bad configuration language design and proper debuggability of system?
* zig test quickstart.zig

