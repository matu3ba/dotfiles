====llms
====artifacts
====known_to_work_use_cases

====llms
* Gemini
* ChatGPT

====artifacts
https://ea.rna.nl/2024/05/27/when-chatgpt-summarises-it-actually-does-nothing-of-the-kind/
* sumarizing
  - parameters from training material
    o subject is well-represented by the parameters => parameters dominate the
    summary more and the actual text you want to summarise influences the summary
    less
    o bad in making specific summaries of a subject that is widespread
  - context (prompts and answers until now in chat) including the text given to summarise
    o context very small => little influence, so result dominated by parameters
    o context large enough and subject not well-represented by params =>
    text to summarize dominates results resulting in "text shortening", not
    true summarizing
  - no intelligence involved, query machine with context for processing

TODO https://news.ycombinator.com/item?id=42100560

====known_to_work_use_cases
given known probability distribution(s) and sufficient handling of edge cases (controlled environment or enumeratable enough)
* 1. copy-paste/use common optimal learned goal
* 2. automate calibration-like tasks
* 3. basic overview stuff not to be trusted/needs validation

any automation system can only be as good as the (formal) semantics encoding
it, so dont expect much without (formal) encoding of meanings or copy-paste
solution.
